{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Acts of Pizza - Baseline Model & EDA\n",
    "## Authors: Ben Arnoldy, Mary Boardman, Zach Merritt, and Kevin Gifford\n",
    "#### Kaggle Competition Description:\n",
    "In machine learning, it is often said there are no free lunches. How wrong we were.\n",
    "\n",
    "This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n",
    "\n",
    "\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\" says one hopeful poster. What about making an algorithm?\n",
    "\n",
    "Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:\n",
    "\n",
    "Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.\n",
    "_______________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Title: EDA & Model Baseline\n",
    "#### Purpose: Load the 'Random Acts of Pizza' train and test data. Conduct an exploratory data analysis to gain an understanding of the data. Create a baseline Logisitic Regression model using non-text (numeric) fields only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load Data and Modules, Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load Data and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from subprocess import check_output\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (4040, 32)\n",
      "Test Shape: (1631, 17)\n"
     ]
    }
   ],
   "source": [
    "#1. Train Data\n",
    "with open('../data/train.json') as fin:\n",
    "    trainjson = json.load(fin)\n",
    "train = pd.io.json.json_normalize(trainjson)\n",
    "#2. Test Data\n",
    "with open('../data/test.json') as fin:\n",
    "    testjson = json.load(fin)\n",
    "test = pd.io.json.json_normalize(testjson)\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Find any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "giver_username_if_known                                    0\n",
       "number_of_downvotes_of_request_at_retrieval                0\n",
       "number_of_upvotes_of_request_at_retrieval                  0\n",
       "post_was_edited                                            0\n",
       "request_id                                                 0\n",
       "request_number_of_comments_at_retrieval                    0\n",
       "request_text                                               0\n",
       "request_text_edit_aware                                    0\n",
       "request_title                                              0\n",
       "requester_account_age_in_days_at_request                   0\n",
       "requester_account_age_in_days_at_retrieval                 0\n",
       "requester_days_since_first_post_on_raop_at_request         0\n",
       "requester_days_since_first_post_on_raop_at_retrieval       0\n",
       "requester_number_of_comments_at_request                    0\n",
       "requester_number_of_comments_at_retrieval                  0\n",
       "requester_number_of_comments_in_raop_at_request            0\n",
       "requester_number_of_comments_in_raop_at_retrieval          0\n",
       "requester_number_of_posts_at_request                       0\n",
       "requester_number_of_posts_at_retrieval                     0\n",
       "requester_number_of_posts_on_raop_at_request               0\n",
       "requester_number_of_posts_on_raop_at_retrieval             0\n",
       "requester_number_of_subreddits_at_request                  0\n",
       "requester_received_pizza                                   0\n",
       "requester_subreddits_at_request                            0\n",
       "requester_upvotes_minus_downvotes_at_request               0\n",
       "requester_upvotes_minus_downvotes_at_retrieval             0\n",
       "requester_upvotes_plus_downvotes_at_request                0\n",
       "requester_upvotes_plus_downvotes_at_retrieval              0\n",
       "requester_user_flair                                    3046\n",
       "requester_username                                         0\n",
       "unix_timestamp_of_request                                  0\n",
       "unix_timestamp_of_request_utc                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data except in column \"requester_user_flair.\" We see in the next section that this isn't a column in the test data, so we may just elect to not use it to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Identify common columns between test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns in train and test:\n",
      "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
      "       'request_title', 'requester_account_age_in_days_at_request',\n",
      "       'requester_days_since_first_post_on_raop_at_request',\n",
      "       'requester_number_of_comments_at_request',\n",
      "       'requester_number_of_comments_in_raop_at_request',\n",
      "       'requester_number_of_posts_at_request',\n",
      "       'requester_number_of_posts_on_raop_at_request',\n",
      "       'requester_number_of_subreddits_at_request',\n",
      "       'requester_subreddits_at_request',\n",
      "       'requester_upvotes_minus_downvotes_at_request',\n",
      "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
      "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'],\n",
      "      dtype='object')\n",
      "----\n",
      "Columns in train but NOT test:\n",
      "Index(['number_of_downvotes_of_request_at_retrieval',\n",
      "       'number_of_upvotes_of_request_at_retrieval', 'post_was_edited',\n",
      "       'request_number_of_comments_at_retrieval', 'request_text',\n",
      "       'requester_account_age_in_days_at_retrieval',\n",
      "       'requester_days_since_first_post_on_raop_at_retrieval',\n",
      "       'requester_number_of_comments_at_retrieval',\n",
      "       'requester_number_of_comments_in_raop_at_retrieval',\n",
      "       'requester_number_of_posts_at_retrieval',\n",
      "       'requester_number_of_posts_on_raop_at_retrieval',\n",
      "       'requester_received_pizza',\n",
      "       'requester_upvotes_minus_downvotes_at_retrieval',\n",
      "       'requester_upvotes_plus_downvotes_at_retrieval',\n",
      "       'requester_user_flair'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Common columns in train and test:\")\n",
    "print(train.columns[train.columns.isin(test.columns)])\n",
    "print(\"----\")\n",
    "print(\"Columns in train but NOT test:\")\n",
    "print(train.columns[~train.columns.isin(test.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, there is a series of columns in the training data only. These columns reflect data about the post (e.g., the #of upvotes) at the time this Reddit data was retrieved. We use certain supervised and unsupversied techniques to derive value from this data (even though that information is not provided on the data set we will be predicting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Create training data, labels, and special 'in training only' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_master = train[['requester_received_pizza']]\n",
    "train_data_master = train[test.columns & train.columns]\n",
    "train_only_data_master = train[train.columns[~train.columns.isin(test.columns)]].drop(['requester_received_pizza'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4040, 32) (4040, 17)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, train_data_master.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Set column types and profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_master = train_data_master.assign(\n",
    "    unix_timestamp_of_request = pd.to_datetime(\n",
    "        train_data_master.unix_timestamp_of_request, unit = \"s\"),\n",
    "    unix_timestamp_of_request_utc = pd.to_datetime(\n",
    "        train_data_master.unix_timestamp_of_request_utc, unit = \"s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4040 entries, 0 to 4039\n",
      "Data columns (total 17 columns):\n",
      "giver_username_if_known                               4040 non-null object\n",
      "request_id                                            4040 non-null object\n",
      "request_text_edit_aware                               4040 non-null object\n",
      "request_title                                         4040 non-null object\n",
      "requester_account_age_in_days_at_request              4040 non-null float64\n",
      "requester_days_since_first_post_on_raop_at_request    4040 non-null float64\n",
      "requester_number_of_comments_at_request               4040 non-null int64\n",
      "requester_number_of_comments_in_raop_at_request       4040 non-null int64\n",
      "requester_number_of_posts_at_request                  4040 non-null int64\n",
      "requester_number_of_posts_on_raop_at_request          4040 non-null int64\n",
      "requester_number_of_subreddits_at_request             4040 non-null int64\n",
      "requester_subreddits_at_request                       4040 non-null object\n",
      "requester_upvotes_minus_downvotes_at_request          4040 non-null int64\n",
      "requester_upvotes_plus_downvotes_at_request           4040 non-null int64\n",
      "requester_username                                    4040 non-null object\n",
      "unix_timestamp_of_request                             4040 non-null datetime64[ns]\n",
      "unix_timestamp_of_request_utc                         4040 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(7), object(6)\n",
      "memory usage: 536.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data_master.describe()\n",
    "train_data_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02972093]\n",
      "[0.03220027 0.02895366]\n",
      "[0.02752027 0.03030987 0.0320975 ]\n",
      "[0.03224385 0.03195599 0.03146872 0.03214503]\n",
      "[0.03220833 0.03220679 0.03185737 0.03202071 0.03186957]\n",
      "[0.03216101 0.03187663 0.03212484 0.03186833 0.03103449 0.0288968 ]\n",
      "[0.03204146 0.03166512 0.02938366 0.03195565 0.03188589 0.03137548\n",
      " 0.02937436]\n",
      "[0.03210404 0.03217974 0.03219331 0.03207929 0.0293139  0.03040083\n",
      " 0.02971743 0.02960569]\n",
      "[0.03219895 0.03210222 0.03051442 0.03132539 0.03166855 0.03084171\n",
      " 0.03222876 0.0307657  0.02871032]\n",
      "[0.02937883 0.03022531 0.02924124 0.03213171 0.03219974 0.03174763\n",
      " 0.03209582 0.02948993 0.03225633 0.03196023]\n",
      "[0.03100062 0.03072237 0.02505238 0.03225769 0.02725056 0.03153503\n",
      " 0.02828998 0.02993152 0.03225787 0.03102434 0.03214588]\n",
      "[0.03188625 0.03162561 0.03190088 0.03071767 0.03219499 0.03126115\n",
      " 0.03169703 0.0318043  0.03218779 0.03158734 0.02903633 0.03222415]\n",
      "[0.0310254  0.03145658 0.03220193 0.03205433 0.03222499 0.02879613\n",
      " 0.03109489 0.03219551 0.02968846 0.03179186 0.03128369 0.0311685\n",
      " 0.02948623]\n",
      "[0.02853178 0.03193733 0.03177715 0.03031135 0.02960395 0.03209557\n",
      " 0.03215172 0.03225597 0.03220877 0.03223921 0.03106164 0.03203166\n",
      " 0.03107276 0.03177634]\n",
      "[0.03222671 0.03219944 0.0317654  0.03218497 0.03143326 0.03204031\n",
      " 0.03048067 0.03214896 0.02809578 0.03195953 0.03225785 0.03172173\n",
      " 0.032245   0.03224525 0.02971009]\n",
      "[0.03221861 0.03225318 0.03222524 0.03186775 0.03043016 0.03201339\n",
      " 0.03152778 0.03120456 0.03211392 0.03208906 0.03203142 0.03206907\n",
      " 0.02992927 0.02533177 0.03133092 0.03184631]\n",
      "[0.03225179 0.03200921 0.03214812 0.03210245 0.02982366 0.02975116\n",
      " 0.02724502 0.03210843 0.03210106 0.0317477  0.0304306  0.02997192\n",
      " 0.03218501 0.03048725 0.02675071 0.02956971 0.0317007 ]\n",
      "[0.03193685 0.03093408 0.03088802 0.03203441 0.03199502 0.03105689\n",
      " 0.03210448 0.03096045 0.03203811 0.03089115 0.03214599 0.02903277\n",
      " 0.03207899 0.03156667 0.0322575  0.0322579  0.03222664 0.03029935]\n",
      "[0.03052503 0.03030239 0.03223995 0.03102583 0.03222606 0.02594802\n",
      " 0.03063165 0.03224764 0.02948525 0.03008284 0.0316988  0.03223583\n",
      " 0.03169636 0.03193148 0.03109082 0.03162347 0.03223767 0.03210192\n",
      " 0.0308216 ]\n",
      "[0.03225646 0.0307468  0.03220499 0.03159153 0.03225172 0.03164009\n",
      " 0.0322258  0.03152574 0.03159596 0.03208916 0.03202366 0.03225096\n",
      " 0.03181712 0.03205373 0.03137837 0.0320728  0.02915687 0.02946919\n",
      " 0.03212389 0.03224368]\n",
      "[0.03176194 0.03197491 0.03194787 0.03165924 0.03221752 0.0321913\n",
      " 0.0308823  0.03131844 0.03219213 0.03117749 0.0318778  0.02449871\n",
      " 0.02974463 0.0319103  0.0309038  0.03225653 0.02713915 0.03221273\n",
      " 0.03144235 0.03225747 0.0283719 ]\n",
      "[0.03198563 0.03222234 0.03116003 0.03148031 0.03018731 0.02928869\n",
      " 0.03121626 0.0289692  0.03194502 0.03225633 0.02830177 0.03209943\n",
      " 0.03179674 0.03203977 0.02882997 0.0321909  0.03147649 0.03146469\n",
      " 0.03224697 0.0319399  0.02883278 0.03193339]\n",
      "[0.03138981 0.0319195  0.03061043 0.02905979 0.02664196 0.03033952\n",
      " 0.0320231  0.03177362 0.02919971 0.03222967 0.03192186 0.03200918\n",
      " 0.03205565 0.03175732 0.03155294 0.03125    0.03213376 0.03208474\n",
      " 0.03219494 0.03221351 0.0321012  0.03143573 0.02924362]\n",
      "[0.03204922 0.02955374 0.0322559  0.03123257 0.03148744 0.03186901\n",
      " 0.03155333 0.03121881 0.03196692 0.02509809 0.03176542 0.02960973\n",
      " 0.03215292 0.03058193 0.03145845 0.03162283 0.03175921 0.03212366\n",
      " 0.03061068 0.03053023 0.03208015 0.03225788 0.03195565 0.03172487]\n",
      "[0.0322485  0.03124731 0.03208015 0.03225238 0.03185137 0.02615369\n",
      " 0.03224967 0.03109741 0.03081721 0.03167637 0.03161154 0.03158552\n",
      " 0.03225735 0.03225804 0.03148786 0.03171701 0.03217294 0.03225777\n",
      " 0.0321632  0.03085534 0.02946176 0.03020802 0.03138219 0.03081428\n",
      " 0.03199277]\n",
      "[0.03200597 0.03151943 0.03222747 0.03194881 0.03225081 0.03215136\n",
      " 0.02981125 0.03089903 0.03222064 0.03023336 0.03216318 0.03216653\n",
      " 0.03206026 0.0305462  0.03170146 0.03125    0.03023435 0.03048531\n",
      " 0.03078379 0.03062323 0.03193175 0.03132366 0.03143396 0.03165564\n",
      " 0.02966419 0.03086943]\n",
      "[0.0311583  0.02933223 0.03184763 0.0307373  0.0282897  0.03224411\n",
      " 0.03157619 0.03187153 0.03192034 0.03148732 0.02679291 0.03223635\n",
      " 0.03223983 0.03136084 0.03217445 0.03208959 0.03224535 0.03162142\n",
      " 0.03125    0.03217115 0.03155327 0.03203554 0.03220663 0.03225045\n",
      " 0.0322555  0.0313706  0.02433712]\n",
      "[0.03141991 0.03225178 0.03225721 0.02682065 0.0321726  0.03196017\n",
      " 0.03143789 0.03126206 0.03157296 0.03223213 0.03200083 0.03212454\n",
      " 0.0303007  0.03200976 0.03007031 0.0314468  0.03150461 0.03115682\n",
      " 0.03175859 0.03224041 0.03225171 0.02912621 0.03224862 0.03225083\n",
      " 0.03225778 0.0315021  0.03126097 0.03125   ]\n",
      "[0.031946   0.03181875 0.03204014 0.03205609 0.03218076 0.03068905\n",
      " 0.030991   0.03176973 0.03179434 0.02880422 0.03225806 0.03218091\n",
      " 0.03101318 0.03171737 0.03170763 0.03163661 0.03116073 0.03089747\n",
      " 0.02984549 0.0291401  0.03084449 0.03207851 0.0306459  0.03213091\n",
      " 0.03225316 0.03166943 0.03208272 0.03196587 0.02538439]\n",
      "[0.03186886 0.03128395 0.03223456 0.03121896 0.03225586 0.03097127\n",
      " 0.0280766  0.03194897 0.03128852 0.0320732  0.03225714 0.03225589\n",
      " 0.02917539 0.0318963  0.02994818 0.02762867 0.03153963 0.03196611\n",
      " 0.03132349 0.03213823 0.03215797 0.03221562 0.03133348 0.03144\n",
      " 0.03056469 0.03204873 0.03225745 0.03125    0.03028527 0.03035503]\n",
      "[0.03179097 0.0321986  0.03212703 0.03217384 0.03067767 0.03165384\n",
      " 0.03163731 0.02942408 0.03221074 0.03223958 0.03225617 0.03191097\n",
      " 0.0319138  0.03013111 0.03217945 0.02940759 0.03146375 0.03038812\n",
      " 0.02946436 0.02549995 0.02938156 0.03225073 0.03125    0.03225806\n",
      " 0.03066797 0.03138714 0.03225017 0.03221646 0.03220743 0.03225806\n",
      " 0.03221662]\n"
     ]
    }
   ],
   "source": [
    "# Ben: I think there's a flaw here, see next module that overwrites this vectorization.\n",
    "\n",
    "# Use a CountVectorizer to vectorize the data, since it is text-based\n",
    "vectorizer = CountVectorizer()\n",
    "vtrain = vectorizer.fit_transform(train)\n",
    "vtest = vectorizer.fit_transform(test)\n",
    "\n",
    "# Since PCA does not work for a sparse matrix, we are using Truncated SVD instead for dimensionality reduction\n",
    "for i in range (1, 32):\n",
    "    svd = TruncatedSVD(n_components = i)\n",
    "    svd.fit(vtrain)\n",
    "    print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Creating Dimension-Reduced Data Sets with Number of Components 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0322565  0.03150892]\n",
      "[0.03225595 0.0265637  0.03225259]\n",
      "[0.03016801 0.03209351 0.02978268 0.03224906]\n",
      "[0.03148288 0.02974733 0.031891   0.0318808  0.03208974]\n",
      "[0.03205637 0.03163417 0.03224267 0.03184984 0.03164383 0.03196845]\n"
     ]
    }
   ],
   "source": [
    "# Instead of a for-loop, we created separate data sets, using separate variables for each. \n",
    "\n",
    "svd_2 = TruncatedSVD(n_components = 2)\n",
    "vtrain_2 = svd_2.fit(vtrain)\n",
    "print(svd_2.explained_variance_ratio_)\n",
    "\n",
    "svd_3 = TruncatedSVD(n_components = 3)\n",
    "vtrain_3 = svd_3.fit(vtrain)\n",
    "print(svd_3.explained_variance_ratio_)\n",
    "\n",
    "svd_4 = TruncatedSVD(n_components = 4)\n",
    "vtrain_4 = svd_4.fit(vtrain)\n",
    "print(svd_4.explained_variance_ratio_)\n",
    "\n",
    "svd_5 = TruncatedSVD(n_components = 5)\n",
    "vtrain_5 = svd_5.fit(vtrain)\n",
    "print(svd_5.explained_variance_ratio_)\n",
    "\n",
    "svd_6 = TruncatedSVD(n_components = 6)\n",
    "vtrain_6 = svd_6.fit(vtrain)\n",
    "print(svd_6.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-31.0, -30.967741935483893, -30.93333333333331, -30.896551724137915, -30.85714285714287, -4.814814814814815, -5.769230769230769, -27.840000000000018, -29.70833333333332, -26.782608695652165, -6.681818181818183, -26.666666666666668, -6.65, -26.52631578947367, -22.666666666666657, -8.470588235294118, -7.5, -22.400000000000002, -7.42857142857143, -7.384615384615386]\n",
      "[-30.868699056028287, -31.014364131139956, -15.259359493253234, -2.424023393990522, -2.1609447091137, -2.0998824859999337, -2.1886766264216724, -0.8461174192206903, -0.7110061954679356, -0.21641691094411608, -0.2798663283512267, -0.2616523233640746, -0.20885440795028443, -0.17120062417388582, -0.13103619040201198, -0.04584915470346862, -0.04645395653576678, -0.03065334708752343, -0.026891911238925315, -0.016888990706392937]\n",
      "[-32.6373854101562, -16.05187425825011, -24.48183047887153, -7.205394018169977, -8.43558717726729, -6.180731779428243, -6.511798585294985, -5.073156928454097, -4.713672601188664, -4.230660421196247, -3.6062348240143325, -1.9232351487558388, -3.2452414363359514, -2.0116936546843243, -1.3793001385550983, -1.7727888435564618, -1.2543175451040933, -0.9748015867105377, -0.6448234406543645, -0.5573733606913782]\n",
      "[-32.72668255532034, -31.547740340852418, -23.78148804941822, -18.005939064427352, -12.329415881424318, -16.190891862037486, -7.518411895960439, -11.967346144442109, -9.054750790616946, -6.437944182423809, -7.393636689590183, -6.337280809672706, -4.3273676336969515, -4.757718159763739, -4.479131423303091, -2.844949893401357, -2.3273873496763278, -2.573872926862914, -2.0563827135380364, -2.064030864257997]\n",
      "[-24.611627800286403, -21.8530885451286, -20.54998470798876, -17.21000805119273, -11.673232592472342, -7.55790491285696, -7.075937829722555, -8.423989541700795, -8.28980963734872, -9.191717821271514, -6.873741317743222, -4.679043546837013, -6.29803775556673, -4.571853533909859, -5.491177879752891, -3.1117053824122665, -4.535432140803661, -3.051661693386765, -2.2432754770409478, -1.974923498613129]\n",
      "[-26.67378214942337, -29.598197947990677, -19.84064157030928, -21.721156418776907, -12.630065564444772, -22.41788896509736, -14.574290035920022, -11.612603841169733, -7.682800470026646, -8.956589660898391, -6.685037012706198, -6.752213438023345, -6.672425085213819, -5.041386836690151, -2.415834502047627, -4.841406728162379, -4.226465193112771, -4.583167087754283, -2.5397965909542903, -2.3406047006383335]\n"
     ]
    }
   ],
   "source": [
    "raw_score = []\n",
    "for i in range(1,21):\n",
    "    kmeans_raw = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans_raw.fit(vtrain)\n",
    "    y_hat_kmeans_raw = kmeans_raw.predict(vtrain)\n",
    "    centers = kmeans_raw.cluster_centers_\n",
    "#     print('The KMeans score for the raw data, using',i,'clusters is:',kmeans_raw.score(vtrain[y_hat_kmeans_raw]))\n",
    "    raw_score.append(kmeans_raw.score(vtrain[y_hat_kmeans_raw]))\n",
    "print(raw_score)\n",
    "\n",
    "svd_2 = TruncatedSVD(n_components = 2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_2 = make_pipeline(svd_2, normalizer)\n",
    "vtrain_lsa2 = lsa_2.fit_transform(vtrain)\n",
    "\n",
    "score_2 = []\n",
    "for i in range(1,21):\n",
    "    kmeans2 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans2.fit(vtrain_lsa2)\n",
    "    y_hat_kmeans2 = kmeans2.predict(vtrain_lsa2)\n",
    "    centers = kmeans2.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 2 component data, using',i,'clusters is:',\n",
    "#           kmeans2.score(vtrain_lsa2[y_hat_kmeans]))\n",
    "    score_2.append(kmeans2.score(vtrain_lsa2[y_hat_kmeans2]))\n",
    "print(score_2)\n",
    "\n",
    "svd_3 = TruncatedSVD(n_components = 3)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_3 = make_pipeline(svd_3, normalizer)\n",
    "vtrain_lsa3 = lsa_3.fit_transform(vtrain)\n",
    "\n",
    "score_3 = []\n",
    "for i in range(1,21):\n",
    "    kmeans3 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans3.fit(vtrain_lsa3)\n",
    "    y_hat_kmeans3 = kmeans3.predict(vtrain_lsa3)\n",
    "    centers = kmeans3.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 3 component data, using',i,'clusters is:',\n",
    "#           kmeans3.score(vtrain_lsa3[y_hat_kmeans3]))\n",
    "    score_3.append(kmeans3.score(vtrain_lsa3[y_hat_kmeans3]))\n",
    "print(score_3)\n",
    "\n",
    "svd_4 = TruncatedSVD(n_components = 4)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_4 = make_pipeline(svd_4, normalizer)\n",
    "vtrain_lsa4 = lsa_4.fit_transform(vtrain)\n",
    "\n",
    "score_4 = []\n",
    "for i in range(1,21):\n",
    "    kmeans4 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans4.fit(vtrain_lsa4)\n",
    "    y_hat_kmeans4 = kmeans4.predict(vtrain_lsa4)\n",
    "    centers = kmeans4.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 4 component data, using',i,'clusters is:',\n",
    "#           kmeans4.score(vtrain_lsa4[y_hat_kmeans4]))\n",
    "    score_4.append(kmeans4.score(vtrain_lsa4[y_hat_kmeans4]))\n",
    "print(score_4)\n",
    "\n",
    "svd_5 = TruncatedSVD(n_components = 5)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_5 = make_pipeline(svd_5, normalizer)\n",
    "vtrain_lsa5 = lsa_5.fit_transform(vtrain)\n",
    "\n",
    "score_5 = []\n",
    "for i in range(1,21):\n",
    "    kmeans5 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans5.fit(vtrain_lsa5)\n",
    "    y_hat_kmeans5 = kmeans5.predict(vtrain_lsa5)\n",
    "    centers = kmeans5.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 5 component data, using',i,'clusters is:',\n",
    "#           kmeans5.score(vtrain_lsa5[y_hat_kmeans5]))\n",
    "    score_5.append(kmeans5.score(vtrain_lsa5[y_hat_kmeans5]))\n",
    "print(score_5)\n",
    "\n",
    "svd_6 = TruncatedSVD(n_components = 6)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_6 = make_pipeline(svd_6, normalizer)\n",
    "vtrain_lsa6 = lsa_6.fit_transform(vtrain)\n",
    "\n",
    "score_6 = []\n",
    "for i in range(1,21):\n",
    "    kmeans6 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans6.fit(vtrain_lsa6)\n",
    "    y_hat_kmeans6 = kmeans6.predict(vtrain_lsa6)\n",
    "    centers = kmeans6.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 6 component data, using',i,'clusters is:',\n",
    "#           kmeans5.score(vtrain_lsa6[y_hat_kmeans6]))\n",
    "    score_6.append(kmeans6.score(vtrain_lsa6[y_hat_kmeans6]))\n",
    "print(score_6)\n",
    "\n",
    "# # The plot below wasn't doing much for me\n",
    "# fig = plt.figure(0,figsize=(32,32))\n",
    "# ax = fig.add_subplot(4,4,2)\n",
    "\n",
    "# # Draw the plots\n",
    "# ax.scatter(vtrain_lsa2[:, 0], vtrain_lsa2[:, 1], s=1, c=y_hat_kmeans)\n",
    "# ax.scatter(centers[:, 0], centers[:, 1], c='red', s=50, alpha=0.6)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.16227048, -2.44084838, -1.77482234, -2.56195806, -2.60549908,\n",
       "       -2.40180912, -2.64288589, -2.21046916, -1.9532393 , -1.7349256 ,\n",
       "       -1.9870991 , -2.06825676, -1.7350605 , -2.63797628, -2.41471406,\n",
       "       -2.64965691, -2.47652578])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model using the raw data fit the train data, but wouldn't predict the test data, as it wasn't the right shape. \n",
    "# I think this goes back to the different dimensionality between the two data sets. \n",
    "# vtraind = vtrain.toarray()\n",
    "# vtestd = vtest.toarray()\n",
    "# gmm = GaussianMixture()\n",
    "# gmm.fit(vtraind)\n",
    "# gmm.predict(vtestd)\n",
    "\n",
    "svd_2 = TruncatedSVD(n_components = 2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_2 = make_pipeline(svd_2, normalizer)\n",
    "vtrain_lsa2 = lsa_2.fit_transform(vtrain)\n",
    "\n",
    "svd_2_test = TruncatedSVD(n_components = 2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_2_test = make_pipeline(svd_2_test, normalizer)\n",
    "vtest_lsa2 = lsa_2_test.fit_transform(vtest)\n",
    "\n",
    "gmm2 = GaussianMixture()\n",
    "gmm2.fit(vtrain_lsa2)\n",
    "gmm2.score_samples(vtest_lsa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-21e9fc05979d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp2pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels_master\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Separate out the train data with negative labels, fit the GMM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_initial_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(X, n_components, n_features)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         raise ValueError('Expected n_samples >= n_components '\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Separate out the train data with positive labels, fit the GMM model\n",
    "p2pos = train[train_labels_master == 1]\n",
    "pos = GaussianMixture(n_components=4, covariance_type='full')\n",
    "pos.fit(p2pos)\n",
    "\n",
    "# Separate out the train data with negative labels, fit the GMM model    \n",
    "p2neg = train[train_labels_master != 1]    \n",
    "neg = GaussianMixture(n_components=4, covariance_type='full')\n",
    "neg.fit(p2neg)\n",
    "\n",
    "# Fit the test data to both models\n",
    "wlprob_pos = pos.score_samples(test)\n",
    "wlprob_neg = neg.score_samples(test)\n",
    "\n",
    "# Create an array that picks the winner from each prediction \n",
    "pred_test_labels = np.array(wlprob_pos > wlprob_neg).astype(int)\n",
    "\n",
    "# Calculate the accuracy to 6 decimal places, print it out\n",
    "accuracy = np.round(np.mean(pred_test_labels == test_labels), 6)   \n",
    "print('The accuracy of this model is:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben's fix attempt\n",
    "I've opted to do this in separate cells just in case I royally screw up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo of Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe we cannot just vectorize the train data because the text-based data\n",
    "# is held within one specific feature called request_test_edit_aware.\n",
    "# Below I repurposed some of Zach's code to vectorize\n",
    "\n",
    "#Create Sparse matrix of words\n",
    "count_vect = CountVectorizer()\n",
    "#Split train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    count_vect.fit_transform(train_data_master['request_text_edit_aware']), \n",
    "    train_labels_master.values.ravel(), test_size=0.2, random_state=0)\n",
    "cv_feature_names = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SVD (in lieu of PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since PCA does not work for a sparse matrix, we are using Truncated SVD instead for dimensionality reduction\n",
    "for top_comps in range (1, 15):\n",
    "    svd = TruncatedSVD(n_components = top_comps)\n",
    "    svd.fit(x_train)\n",
    "    print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of vectorizer shows a major dropoff in explained variance after the 1st component\n",
    "# This suggests that projecting down to 1 component might be best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try various hyperparameters with GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gmm(svd_comps = 2, gmm_comps = 4, cov_type = 'full', sparse = True):\n",
    "    \n",
    "    # project data to few dimensions using SVD if sparse, PCA if dense\n",
    "    if (sparse): svd_test = TruncatedSVD(n_components=svd_comps)\n",
    "    else: svd_test = PCA(n_components=svd_comps)\n",
    "    svd_test_ft = svd_test.fit_transform(x_test)\n",
    "    if (sparse): svd_train = TruncatedSVD(n_components=svd_comps)\n",
    "    else: svd_train = PCA(n_components=svd_comps)\n",
    "    svd_train_ft = svd_train.fit_transform(x_train)\n",
    "\n",
    "    # create two sets of svd data, one that's positively labeled, one that's negatively labeled\n",
    "    pos_svd = svd_train_ft[y_train == 1]\n",
    "    neg_svd = svd_train_ft[y_train == 0]\n",
    "    \n",
    "    # fit a GMM for pos and neg datasets\n",
    "    gmm_pos = GaussianMixture(n_components=gmm_comps, covariance_type=cov_type) \n",
    "    gmm_fit_pos = gmm_pos.fit(pos_svd)\n",
    "    gmm_neg = GaussianMixture(n_components=gmm_comps, covariance_type=cov_type) \n",
    "    gmm_fit_neg = gmm_neg.fit(neg_svd)\n",
    "\n",
    "    prediction = np.ndarray(shape=y_test.shape)\n",
    "    \n",
    "    for sample in range(svd_test_ft.shape[0]):\n",
    "        pos_score = gmm_fit_pos.score(svd_test_ft[sample].reshape(1,-1))\n",
    "        neg_score = gmm_fit_neg.score(svd_test_ft[sample].reshape(1, -1))\n",
    "        # make pick\n",
    "        if (pos_score >= neg_score): prediction[sample] = 1\n",
    "        else: prediction[sample] = 0 \n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(y_test, prediction)\n",
    "    # f1 = metrics.f1_score(y_test, prediction)\n",
    "    print(\"Accuracy score for\", svd_comps,\"SVD comps,\",gmm_comps,\"GMM comps,\",\"Covariance type\",cov_type,\"=\",accuracy)\n",
    "\n",
    "    return(accuracy)\n",
    "\n",
    "def gmm_trials(sparse = True, max_params = 50):\n",
    "\n",
    "    def calc_limits(max_params = 50):\n",
    "        # this function helps estaish the svd and gmm settings to stay within max_params\n",
    "\n",
    "        valid_configs = [] # list of tuples (num_svd_components, num_gmm_components)\n",
    "\n",
    "        for cov_type in ['spherical', 'diag', 'tied', 'full']:\n",
    "            for svd_comp in range(1,20): \n",
    "                for gmm_comp in range(1,20): \n",
    "                    if ((svd_comp + svd_comp) * gmm_comp) * 2 <= max_params:\n",
    "                        valid_configs.append((svd_comp, gmm_comp, cov_type))\n",
    "\n",
    "        return(valid_configs)\n",
    "\n",
    "    configs = calc_limits(max_params) # get the valid configurations\n",
    "\n",
    "    top_accuracy_val = 0 # keeps track of top accuracy value\n",
    "    top_accuracy_config = () # keeps track of config that creates top accuracy value\n",
    "    \n",
    "    for config in configs:\n",
    "        accuracy = train_gmm(config[0], config[1], config[2])\n",
    "        if accuracy > top_accuracy_val:\n",
    "            top_accuracy_val = accuracy\n",
    "            top_accuracy_config = config\n",
    "\n",
    "    print(\"*********************************************************************************\")\n",
    "    print(\"The best accuracy score is\", top_accuracy_val)\n",
    "    print(\"To get it, set SVD comps to\", top_accuracy_config[0], \", GMM comps to\", top_accuracy_config[1],\", and covariance type to\", top_accuracy_config[2])\n",
    "\n",
    "gmm_trials(sparse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Dimensionality reduction on the vectorized text field doesn't help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: Dimensionality reduction on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using ALL features, even those not in the test data. \n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler = min_max_scaler.fit_transform(\n",
    "    train.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    mn_mx_scaler, \n",
    "    train_labels_master.values.ravel(), test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_trials(sparse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using just the features in common with train and test. \n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler = min_max_scaler.fit_transform(\n",
    "    train_data_master.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    mn_mx_scaler, \n",
    "    train_labels_master.values.ravel(), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_trials(sparse = False, max_params = 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce vectorized word field, plug it into logistic regression with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Sparse matrix of words\n",
    "count_vect = CountVectorizer()\n",
    "count_vect_ft = count_vect.fit_transform(train_data_master['request_text_edit_aware'])\n",
    "\n",
    "#Reduce the vectorized word feature to small number of numerical components\n",
    "svd_comps = 5 # after some trial and error, this was the best\n",
    "svd = TruncatedSVD(n_components=svd_comps)\n",
    "svd_ft = svd.fit_transform(count_vect_ft)\n",
    "\n",
    "# add the newly reduced text field feature to the train and test data\n",
    "train_data_SVD = train_data_master.copy(deep = True)\n",
    "for comp in range(svd_comps):\n",
    "    train_data_SVD['SVD'+str(comp)] = svd_ft[:,comp]\n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler = min_max_scaler.fit_transform(train_data_SVD.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    mn_mx_scaler, \n",
    "    train_labels_master.values.ravel(), test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do logistic regression on our new train data\n",
    "\n",
    "def model_report(title, y_test, predictions):\n",
    "\n",
    "    \"\"\"\n",
    "    Output: Classification report, confusion matrix, and ROC curve\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    print(\"---------\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    all_sample_title = 'Accuracy: {0}'.format(round(metrics.accuracy_score(y_test, predictions),2))\n",
    "    plt.title(all_sample_title, size = 15)\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "#Train Model\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "#Output model report\n",
    "model_report(\"Logistic Regression (using only common numeric fields)\",y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* Reducing the vectorized word feature got us to 0.68 accuracy - the exact same accuracy that Naive Bayes got us. \n",
    "* Reducing all the features (even those not in the test set) got us to 0.72 accuracy - a bit less than the 0.75 accuracy from logistic regression on the features. \n",
    "* Reducing just the features that are shared by train and test sets got us to 0.71 accuracy - even further from the 0.75 accuracy of logistic regression.\n",
    "* Vectorizing the text field, reducing it through SVD to 5 components, adding it to the train data, and running logistic regression again did improve accuracy by small margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kevin's Misadventures in Feature Imputation\n",
    "EDA and experiments in predicting the features that are included in training data, but missing from test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis of Missing Features\n",
    "As identified in the initial EDA, the training data contains a number of features that are not present in the test data (beyond whether or not a pizza was received). We would like to explore whether predicting any of these missing feature values can be used to improve the accuracy of our classification predictions. The first step is examining the relationship between these features and the sucess of the pizza request.\n",
    "Because most of these \"missing\" features are continuous variables, and our success/failure variable is binary, we'll start our exploration by running a Point-Biserial correlation analysis for each feature of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# List the numeric columns that are in the training data, but not in the test data\n",
    "numeric_cols = ['number_of_downvotes_of_request_at_retrieval',\n",
    "       'number_of_upvotes_of_request_at_retrieval',\n",
    "       'request_number_of_comments_at_retrieval',\n",
    "       'requester_account_age_in_days_at_retrieval',\n",
    "       'requester_days_since_first_post_on_raop_at_retrieval',\n",
    "       'requester_number_of_comments_at_retrieval',\n",
    "       'requester_number_of_comments_in_raop_at_retrieval',\n",
    "       'requester_number_of_posts_at_retrieval',\n",
    "       'requester_number_of_posts_on_raop_at_retrieval',\n",
    "       'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "       'requester_upvotes_plus_downvotes_at_retrieval']\n",
    "\n",
    "# Iterate through the column list and evaluate point-biserial correlation with whether a pizza was received.\n",
    "results = []\n",
    "for col in numeric_cols:\n",
    "    result = stats.pointbiserialr(train[col],train['requester_received_pizza'])\n",
    "    temp = list((col,result[0],result[1]))\n",
    "    results.append(temp)\n",
    "\n",
    "df_corr = pd.DataFrame(results)\n",
    "df_corr.columns = ['Feature Name','Correlation','p-value']\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation analysis, most of the \"missing\" features are very weakly correlated with whether the requestor received a pizza. The strongest relationships are with the number of comments on the request (r = 0.289), the number of comments the requestor has made in the RAOP subreddit (r = 0.274), and the number of posts the requestor has made in the RAOP subreddit (r = 0.461). \n",
    "Based on these relationships, we'll isolate these three variables for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Missing Feature Values\n",
    "Attempting to impute missing features using a multioutput random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data prediction score: 36.27%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "regr_multirf = RandomForestRegressor(max_depth=30,random_state=0)\n",
    "\n",
    "# cols_to_predict = list(('request_number_of_comments_at_retrieval','requester_number_of_comments_in_raop_at_retrieval','requester_number_of_posts_on_raop_at_retrieval'))\n",
    "cols_to_predict = 'requester_number_of_posts_on_raop_at_retrieval'\n",
    "impute_train_data = train[test.columns]\n",
    "impute_train_labels = train[cols_to_predict]\n",
    "# impute_train_labels\n",
    "impute_train_scaled = min_max_scaler.fit_transform(impute_train_data.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "x_impute_train, x_impute_test, y_impute_train, y_impute_test = train_test_split(impute_train_scaled, impute_train_labels.values, test_size=0.2, random_state=0)\n",
    "\n",
    "# y_impute_train.shape\n",
    "                                    \n",
    "regr_multirf.fit(x_impute_train, y_impute_train)\n",
    "score = regr_multirf.score(x_impute_test, y_impute_test)\n",
    "print(\"Test data prediction score: {:.2f}%\".format(score*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
