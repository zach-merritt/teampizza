{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza - Baseline Model & EDA\n",
    "## Authors: Ben Arnoldy, Mary Boardman, Zach Merritt, and Kevin Gifford\n",
    "#### Kaggle Competition Description:\n",
    "In machine learning, it is often said there are no free lunches. How wrong we were.\n",
    "\n",
    "This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n",
    "\n",
    "\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\" says one hopeful poster. What about making an algorithm?\n",
    "\n",
    "Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:\n",
    "\n",
    "Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.\n",
    "_______________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Title: EDA & Model Baseline\n",
    "#### Purpose: Load the 'Random Acts of Pizza' train and test data. Conduct an exploratory data analysis to gain an understanding of the data. Create a baseline Logisitic Regression model using non-text (numeric) fields only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load Data and Modules, Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load Data and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (4040, 32)\n",
      "Test Shape: (1631, 17)\n"
     ]
    }
   ],
   "source": [
    "#1. Train Data\n",
    "with open('../data/train.json') as fin:\n",
    "    trainjson = json.load(fin)\n",
    "train = pd.io.json.json_normalize(trainjson)\n",
    "#2. Test Data\n",
    "with open('../data/test.json') as fin:\n",
    "    testjson = json.load(fin)\n",
    "test = pd.io.json.json_normalize(testjson)\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Find any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "giver_username_if_known                                    0\n",
       "number_of_downvotes_of_request_at_retrieval                0\n",
       "number_of_upvotes_of_request_at_retrieval                  0\n",
       "post_was_edited                                            0\n",
       "request_id                                                 0\n",
       "request_number_of_comments_at_retrieval                    0\n",
       "request_text                                               0\n",
       "request_text_edit_aware                                    0\n",
       "request_title                                              0\n",
       "requester_account_age_in_days_at_request                   0\n",
       "requester_account_age_in_days_at_retrieval                 0\n",
       "requester_days_since_first_post_on_raop_at_request         0\n",
       "requester_days_since_first_post_on_raop_at_retrieval       0\n",
       "requester_number_of_comments_at_request                    0\n",
       "requester_number_of_comments_at_retrieval                  0\n",
       "requester_number_of_comments_in_raop_at_request            0\n",
       "requester_number_of_comments_in_raop_at_retrieval          0\n",
       "requester_number_of_posts_at_request                       0\n",
       "requester_number_of_posts_at_retrieval                     0\n",
       "requester_number_of_posts_on_raop_at_request               0\n",
       "requester_number_of_posts_on_raop_at_retrieval             0\n",
       "requester_number_of_subreddits_at_request                  0\n",
       "requester_received_pizza                                   0\n",
       "requester_subreddits_at_request                            0\n",
       "requester_upvotes_minus_downvotes_at_request               0\n",
       "requester_upvotes_minus_downvotes_at_retrieval             0\n",
       "requester_upvotes_plus_downvotes_at_request                0\n",
       "requester_upvotes_plus_downvotes_at_retrieval              0\n",
       "requester_user_flair                                    3046\n",
       "requester_username                                         0\n",
       "unix_timestamp_of_request                                  0\n",
       "unix_timestamp_of_request_utc                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data except in column \"requester_user_flair.\" We see in the next section that this isn't a column in the test data, so we may just elect to not use it to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Identify common columns between test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns in train and test:\n",
      "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
      "       'request_title', 'requester_account_age_in_days_at_request',\n",
      "       'requester_days_since_first_post_on_raop_at_request',\n",
      "       'requester_number_of_comments_at_request',\n",
      "       'requester_number_of_comments_in_raop_at_request',\n",
      "       'requester_number_of_posts_at_request',\n",
      "       'requester_number_of_posts_on_raop_at_request',\n",
      "       'requester_number_of_subreddits_at_request',\n",
      "       'requester_subreddits_at_request',\n",
      "       'requester_upvotes_minus_downvotes_at_request',\n",
      "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
      "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'],\n",
      "      dtype='object')\n",
      "----\n",
      "Columns in train but NOT test:\n",
      "Index(['number_of_downvotes_of_request_at_retrieval',\n",
      "       'number_of_upvotes_of_request_at_retrieval', 'post_was_edited',\n",
      "       'request_number_of_comments_at_retrieval', 'request_text',\n",
      "       'requester_account_age_in_days_at_retrieval',\n",
      "       'requester_days_since_first_post_on_raop_at_retrieval',\n",
      "       'requester_number_of_comments_at_retrieval',\n",
      "       'requester_number_of_comments_in_raop_at_retrieval',\n",
      "       'requester_number_of_posts_at_retrieval',\n",
      "       'requester_number_of_posts_on_raop_at_retrieval',\n",
      "       'requester_received_pizza',\n",
      "       'requester_upvotes_minus_downvotes_at_retrieval',\n",
      "       'requester_upvotes_plus_downvotes_at_retrieval',\n",
      "       'requester_user_flair'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Common columns in train and test:\")\n",
    "print(train.columns[train.columns.isin(test.columns)])\n",
    "print(\"----\")\n",
    "print(\"Columns in train but NOT test:\")\n",
    "print(train.columns[~train.columns.isin(test.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, there is a series of columns in the training data only. These columns reflect data about the post (e.g., the #of upvotes) at the time this Reddit data was retrieved. We use certain supervised and unsupversied techniques to derive value from this data (even though that information is not provided on the data set we will be predicting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Create training data, labels, and special 'in training only' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_master = train[['requester_received_pizza']]\n",
    "train_data_master = train[test.columns & train.columns]\n",
    "train_only_data_master = train[train.columns[~train.columns.isin(test.columns)]].drop(['requester_received_pizza'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Set column types and profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_master = train_data_master.assign(\n",
    "    unix_timestamp_of_request = pd.to_datetime(\n",
    "        train_data_master.unix_timestamp_of_request, unit = \"s\"),\n",
    "    unix_timestamp_of_request_utc = pd.to_datetime(\n",
    "        train_data_master.unix_timestamp_of_request_utc, unit = \"s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4040 entries, 0 to 4039\n",
      "Data columns (total 17 columns):\n",
      "giver_username_if_known                               4040 non-null object\n",
      "request_id                                            4040 non-null object\n",
      "request_text_edit_aware                               4040 non-null object\n",
      "request_title                                         4040 non-null object\n",
      "requester_account_age_in_days_at_request              4040 non-null float64\n",
      "requester_days_since_first_post_on_raop_at_request    4040 non-null float64\n",
      "requester_number_of_comments_at_request               4040 non-null int64\n",
      "requester_number_of_comments_in_raop_at_request       4040 non-null int64\n",
      "requester_number_of_posts_at_request                  4040 non-null int64\n",
      "requester_number_of_posts_on_raop_at_request          4040 non-null int64\n",
      "requester_number_of_subreddits_at_request             4040 non-null int64\n",
      "requester_subreddits_at_request                       4040 non-null object\n",
      "requester_upvotes_minus_downvotes_at_request          4040 non-null int64\n",
      "requester_upvotes_plus_downvotes_at_request           4040 non-null int64\n",
      "requester_username                                    4040 non-null object\n",
      "unix_timestamp_of_request                             4040 non-null datetime64[ns]\n",
      "unix_timestamp_of_request_utc                         4040 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(7), object(6)\n",
      "memory usage: 536.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data_master.describe()\n",
    "train_data_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03224117]\n",
      "[ 0.03224219  0.0304133 ]\n",
      "[ 0.02992198  0.02807986  0.0305983 ]\n",
      "[ 0.03197376  0.03217411  0.03178523  0.03225802]\n",
      "[ 0.02752933  0.0317864   0.03224705  0.03097403  0.03101596]\n",
      "[ 0.03075156  0.03211839  0.02910555  0.03221067  0.0269372   0.02987501]\n",
      "[ 0.03209292  0.02900304  0.03221527  0.03225631  0.03177776  0.03186617\n",
      "  0.02776758]\n",
      "[ 0.03145666  0.02913705  0.03225681  0.03031721  0.03130617  0.03223785\n",
      "  0.03093214  0.03198467]\n",
      "[ 0.03222603  0.03003651  0.03131154  0.03225538  0.03224564  0.03195873\n",
      "  0.02689188  0.03225771  0.03194264]\n",
      "[ 0.03186377  0.03219792  0.02789387  0.03130725  0.02995907  0.03222589\n",
      "  0.03225795  0.03220399  0.0310018   0.03223042]\n",
      "[ 0.03225764  0.0298621   0.03223184  0.03214671  0.03198844  0.03196338\n",
      "  0.03189147  0.03205516  0.03146474  0.03101119  0.03209509]\n",
      "[ 0.02891589  0.03180612  0.03186347  0.03225229  0.0320773   0.03219261\n",
      "  0.03157722  0.03208945  0.03208882  0.0318493   0.03104503  0.03201184]\n",
      "[ 0.03124322  0.03215426  0.03134105  0.02954768  0.03201924  0.03192112\n",
      "  0.03082454  0.03221244  0.02937378  0.03117433  0.03225575  0.03054661\n",
      "  0.03225774]\n",
      "[ 0.03159447  0.03085165  0.0315349   0.03202505  0.03205605  0.02987309\n",
      "  0.03220386  0.03212234  0.0295631   0.03225387  0.03225075  0.02731434\n",
      "  0.03119926  0.03221127]\n",
      "[ 0.03187258  0.03198637  0.03065789  0.03150233  0.03104357  0.03015521\n",
      "  0.03224166  0.03214033  0.03224475  0.03209195  0.03017335  0.02969159\n",
      "  0.03163421  0.03190289  0.03203864]\n",
      "[ 0.03201278  0.03225708  0.03169848  0.03077442  0.03101586  0.03196267\n",
      "  0.03223143  0.02878608  0.03162678  0.03199564  0.02758977  0.03088574\n",
      "  0.03149825  0.03026571  0.03035903  0.03221019]\n",
      "[ 0.03082295  0.02819384  0.02862215  0.02835954  0.03204345  0.03222098\n",
      "  0.03224158  0.0310208   0.03202324  0.03156003  0.03164594  0.03141522\n",
      "  0.03213212  0.03214842  0.02811972  0.02901509  0.03225428]\n",
      "[ 0.0277034   0.03074265  0.03204083  0.03165931  0.0311886   0.03140311\n",
      "  0.03188708  0.03188002  0.03218636  0.02989978  0.03209298  0.03072753\n",
      "  0.03168854  0.03032629  0.03204419  0.03124836  0.03175703  0.03070664]\n",
      "[ 0.032088    0.03197207  0.0322446   0.03119766  0.03225475  0.03057879\n",
      "  0.03119791  0.03143381  0.03225546  0.03199951  0.03020699  0.03168027\n",
      "  0.03157222  0.03143464  0.03104359  0.03029584  0.03169251  0.02717154\n",
      "  0.03140944]\n",
      "[ 0.03192043  0.0312251   0.0298362   0.03083452  0.03224463  0.03201165\n",
      "  0.03225311  0.03150667  0.03156429  0.03133327  0.03097187  0.02753985\n",
      "  0.03166178  0.02681153  0.0319364   0.03224369  0.03220954  0.03216596\n",
      "  0.03217907  0.03210672]\n",
      "[ 0.03218847  0.0322535   0.03225751  0.03217711  0.03223924  0.03198778\n",
      "  0.03182117  0.03187672  0.03184781  0.03198923  0.0294899   0.03126283\n",
      "  0.03211132  0.02811165  0.03224253  0.02952819  0.03014945  0.03117114\n",
      "  0.03112449  0.03192344  0.03221736]\n",
      "[ 0.03224606  0.03041324  0.03214846  0.03218046  0.03177678  0.03201226\n",
      "  0.03215528  0.03054444  0.02930489  0.03216505  0.02976156  0.03187454\n",
      "  0.02786051  0.03200048  0.03158377  0.02987196  0.03198418  0.03222402\n",
      "  0.02991309  0.03224254  0.03093687  0.02919345]\n",
      "[ 0.03120978  0.03222771  0.03197167  0.03187924  0.02623342  0.03175504\n",
      "  0.03223016  0.03191105  0.03101292  0.02661461  0.03094973  0.03161503\n",
      "  0.03129843  0.03209597  0.03018538  0.03125     0.03189532  0.02994302\n",
      "  0.03026099  0.03134604  0.03149076  0.03196063  0.03181408]\n",
      "[ 0.03118769  0.03063549  0.02799256  0.03159069  0.03222726  0.03131029\n",
      "  0.03204109  0.03160536  0.03192655  0.03049235  0.03207799  0.03089196\n",
      "  0.03174935  0.03214222  0.03183961  0.03219128  0.03124362  0.03154786\n",
      "  0.03222476  0.03125     0.03224576  0.02890805  0.02825376  0.03209119]\n",
      "[ 0.03136364  0.03225674  0.03137588  0.03146492  0.02662629  0.03118953\n",
      "  0.03212143  0.0318475   0.02830985  0.03056799  0.03214418  0.03134918\n",
      "  0.03198872  0.03102641  0.03134853  0.03167479  0.03224351  0.03179748\n",
      "  0.03125     0.03131721  0.0322466   0.03224135  0.0302894   0.03224985\n",
      "  0.03011897]\n",
      "[ 0.02727356  0.03224636  0.03224268  0.0313758   0.03225775  0.02919737\n",
      "  0.03213377  0.03211538  0.03176539  0.03224279  0.03212478  0.0319101\n",
      "  0.03095177  0.03036037  0.03014509  0.03218427  0.02791279  0.03148562\n",
      "  0.03180801  0.03209324  0.03199156  0.03223108  0.03125     0.03222693\n",
      "  0.03120573  0.03092531]\n",
      "[ 0.03223903  0.03140173  0.03016873  0.03218424  0.03113528  0.03218275\n",
      "  0.03130138  0.0296481   0.03225632  0.03129752  0.03139188  0.03065634\n",
      "  0.02814763  0.03197245  0.03222224  0.03123541  0.0321357   0.03121368\n",
      "  0.03147705  0.03224926  0.03089206  0.03160219  0.03225789  0.03125\n",
      "  0.02995837  0.03121641  0.03053724]\n",
      "[ 0.03117297  0.03218897  0.03225749  0.03098496  0.02683156  0.03192963\n",
      "  0.0319076   0.0321916   0.0312637   0.03029318  0.03218721  0.0280366\n",
      "  0.03173191  0.03158289  0.03193242  0.0304444   0.03053566  0.03223481\n",
      "  0.03221044  0.03125     0.03225313  0.03104704  0.03224014  0.03099643\n",
      "  0.03192504  0.03222323  0.03142799  0.02715854]\n",
      "[ 0.03166952  0.03198287  0.03196107  0.03217807  0.03176618  0.03224776\n",
      "  0.03205583  0.03153262  0.03086101  0.03142253  0.02902972  0.03168566\n",
      "  0.0318893   0.03028176  0.03187837  0.03159782  0.03190195  0.03186479\n",
      "  0.03169494  0.03125     0.03141853  0.03174826  0.02765216  0.03196918\n",
      "  0.03073247  0.03179272  0.03224629  0.02862672  0.02711666]\n",
      "[ 0.0310746   0.03200389  0.03221605  0.02465735  0.03225117  0.03125\n",
      "  0.0322573   0.03174491  0.03023811  0.03190151  0.03225474  0.03103874\n",
      "  0.031363    0.0318991   0.0316179   0.02903193  0.03212402  0.03212738\n",
      "  0.02723679  0.0310504   0.03215435  0.03207476  0.03222197  0.03083554\n",
      "  0.03224346  0.0321892   0.03153313  0.0314554   0.03216874  0.03171393]\n",
      "[ 0.03094477  0.03216533  0.03043823  0.03217664  0.03017431  0.032053\n",
      "  0.03214727  0.03225499  0.03223551  0.032191    0.03210818  0.0304357\n",
      "  0.03217563  0.03039306  0.03146507  0.03020196  0.03157467  0.03225333\n",
      "  0.03217022  0.02721534  0.03224975  0.03225642  0.0322554   0.03125\n",
      "  0.03160935  0.03014182  0.03105516  0.02756999  0.02931904  0.03148221\n",
      "  0.03179961]\n"
     ]
    }
   ],
   "source": [
    "# Use a CountVectorizer to vectorize the data, since it is text-based\n",
    "vectorizer = CountVectorizer()\n",
    "vtrain = vectorizer.fit_transform(train)\n",
    "vtest = vectorizer.fit_transform(test)\n",
    "\n",
    "# Since PCA does not work for a sparse matrix, we are using Truncated SVD instead for dimensionality reduction\n",
    "for i in range (1, 32):\n",
    "    svd = TruncatedSVD(n_components = i)\n",
    "    svd.fit(vtrain)\n",
    "    print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Creating Dimension-Reduced Data Sets with Number of Components 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0322521   0.03203267]\n",
      "[ 0.03038149  0.03224394  0.03212615]\n",
      "[ 0.03216073  0.03181132  0.0322574   0.03195261]\n",
      "[ 0.02992446  0.0320203   0.03225629  0.03210184  0.03220781]\n",
      "[ 0.02920177  0.02920185  0.03142868  0.03000636  0.03091608  0.03206158]\n"
     ]
    }
   ],
   "source": [
    "# Instead of a for-loop, we created separate data sets, using separate variables for each. \n",
    "\n",
    "svd_2 = TruncatedSVD(n_components = 2)\n",
    "vtrain_2 = svd_2.fit(vtrain)\n",
    "print(svd_2.explained_variance_ratio_)\n",
    "\n",
    "svd_3 = TruncatedSVD(n_components = 3)\n",
    "vtrain_3 = svd_3.fit(vtrain)\n",
    "print(svd_3.explained_variance_ratio_)\n",
    "\n",
    "svd_4 = TruncatedSVD(n_components = 4)\n",
    "vtrain_4 = svd_4.fit(vtrain)\n",
    "print(svd_4.explained_variance_ratio_)\n",
    "\n",
    "svd_5 = TruncatedSVD(n_components = 5)\n",
    "vtrain_5 = svd_5.fit(vtrain)\n",
    "print(svd_5.explained_variance_ratio_)\n",
    "\n",
    "svd_6 = TruncatedSVD(n_components = 6)\n",
    "vtrain_6 = svd_6.fit(vtrain)\n",
    "print(svd_6.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-31.0, -30.967741935483893, -30.93333333333331, -30.896551724137915, -30.85714285714287, -29.85185185185184, -28.846153846153825, -4.8, -6.708333333333333, -27.7391304347826, -6.681818181818183, -28.571428571428573, -7.6000000000000005, -7.578947368421053, -10.38888888888889, -6.588235294117648, -23.4375, -18.666666666666668, -18.57142857142857, -8.307692307692308]\n",
      "[-31.381506843869321, -14.997176071877483, -9.3432309570669858, -4.26587106570455, -3.160970212564111, -2.2801263787698316, -1.1013204986730025, -0.78767549045146401, -0.67964914781019481, -0.83807499302375899, -0.43500721155880928, -0.40902992260446691, -0.20883457038090492, -0.22936579346328256, -0.33493696726466982, -0.10936210689692027, -0.097280104990433269, -0.052811559813683573, -0.040251136497941653, -0.036245767758011582]\n",
      "[-36.545820849825247, -18.391363441918415, -10.892638928755609, -15.076596184369798, -16.307152193077577, -10.498511174481742, -9.0304917349591989, -5.1594608451421138, -6.1096643494572636, -3.3092348578311661, -3.4355887014371151, -2.2436200235924404, -2.9751268925897327, -2.4567161830151201, -1.77290187553043, -1.6575787609724015, -1.7704021406949881, -0.8861307626590561, -0.88712309551296931, -0.68074978148717891]\n",
      "[-26.999285740602545, -22.077136264442231, -19.743578248772398, -21.52801277406741, -12.11573085671348, -7.3664055426915915, -7.3662235229899942, -8.5434652466261962, -5.699628928458111, -7.2751025022901388, -5.6205664767211125, -6.1777721744079823, -3.9793566178316255, -7.3643597563910728, -2.785334233104646, -3.563056830170769, -2.5220320457101733, -2.1022359394884758, -2.2843375163863517, -2.0837669228459332]\n",
      "[-27.651899003624067, -14.724332737186163, -14.938746713711822, -12.994469009761811, -11.257341119196591, -10.224089408103808, -14.421984524688053, -10.727598106026443, -6.7931145341045163, -7.339567829618554, -8.0369574435196576, -5.8637822613020552, -5.2052941330895006, -5.386370626371046, -5.3234785429842457, -3.4393560092584794, -2.7367758275573193, -3.0939000132887631, -2.1695513475183144, -2.1937226697360623]\n",
      "[-27.982613816149726, -18.633526713085054, -18.711326053321194, -17.415340199169243, -12.706768978513114, -12.223164932922414, -10.588247522491148, -13.572623440606673, -9.2595172324079105, -7.8697686024957267, -7.3164615896390686, -8.2165781664004278, -7.0377970653740638, -5.3670774761580082, -6.7564958867343865, -4.5711655242801603, -3.3209704737386052, -4.4040272978768069, -3.6772109838581302, -2.8139435734437006]\n"
     ]
    }
   ],
   "source": [
    "raw_score = []\n",
    "for i in range(1,21):\n",
    "    kmeans_raw = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans_raw.fit(vtrain)\n",
    "    y_hat_kmeans_raw = kmeans_raw.predict(vtrain)\n",
    "    centers = kmeans_raw.cluster_centers_\n",
    "#     print('The KMeans score for the raw data, using',i,'clusters is:',kmeans_raw.score(vtrain[y_hat_kmeans_raw]))\n",
    "    raw_score.append(kmeans_raw.score(vtrain[y_hat_kmeans_raw]))\n",
    "print(raw_score)\n",
    "\n",
    "svd_2 = TruncatedSVD(n_components = 2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_2 = make_pipeline(svd_2, normalizer)\n",
    "vtrain_lsa2 = lsa_2.fit_transform(vtrain)\n",
    "\n",
    "score_2 = []\n",
    "for i in range(1,21):\n",
    "    kmeans2 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans2.fit(vtrain_lsa2)\n",
    "    y_hat_kmeans2 = kmeans2.predict(vtrain_lsa2)\n",
    "    centers = kmeans2.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 2 component data, using',i,'clusters is:',\n",
    "#           kmeans2.score(vtrain_lsa2[y_hat_kmeans]))\n",
    "    score_2.append(kmeans2.score(vtrain_lsa2[y_hat_kmeans2]))\n",
    "print(score_2)\n",
    "\n",
    "svd_3 = TruncatedSVD(n_components = 3)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_3 = make_pipeline(svd_3, normalizer)\n",
    "vtrain_lsa3 = lsa_3.fit_transform(vtrain)\n",
    "\n",
    "score_3 = []\n",
    "for i in range(1,21):\n",
    "    kmeans3 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans3.fit(vtrain_lsa3)\n",
    "    y_hat_kmeans3 = kmeans3.predict(vtrain_lsa3)\n",
    "    centers = kmeans3.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 3 component data, using',i,'clusters is:',\n",
    "#           kmeans3.score(vtrain_lsa3[y_hat_kmeans3]))\n",
    "    score_3.append(kmeans3.score(vtrain_lsa3[y_hat_kmeans3]))\n",
    "print(score_3)\n",
    "\n",
    "svd_4 = TruncatedSVD(n_components = 4)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_4 = make_pipeline(svd_4, normalizer)\n",
    "vtrain_lsa4 = lsa_4.fit_transform(vtrain)\n",
    "\n",
    "score_4 = []\n",
    "for i in range(1,21):\n",
    "    kmeans4 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans4.fit(vtrain_lsa4)\n",
    "    y_hat_kmeans4 = kmeans4.predict(vtrain_lsa4)\n",
    "    centers = kmeans4.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 4 component data, using',i,'clusters is:',\n",
    "#           kmeans4.score(vtrain_lsa4[y_hat_kmeans4]))\n",
    "    score_4.append(kmeans4.score(vtrain_lsa4[y_hat_kmeans4]))\n",
    "print(score_4)\n",
    "\n",
    "svd_5 = TruncatedSVD(n_components = 5)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_5 = make_pipeline(svd_5, normalizer)\n",
    "vtrain_lsa5 = lsa_5.fit_transform(vtrain)\n",
    "\n",
    "score_5 = []\n",
    "for i in range(1,21):\n",
    "    kmeans5 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans5.fit(vtrain_lsa5)\n",
    "    y_hat_kmeans5 = kmeans5.predict(vtrain_lsa5)\n",
    "    centers = kmeans5.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 5 component data, using',i,'clusters is:',\n",
    "#           kmeans5.score(vtrain_lsa5[y_hat_kmeans5]))\n",
    "    score_5.append(kmeans5.score(vtrain_lsa5[y_hat_kmeans5]))\n",
    "print(score_5)\n",
    "\n",
    "svd_6 = TruncatedSVD(n_components = 6)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_6 = make_pipeline(svd_6, normalizer)\n",
    "vtrain_lsa6 = lsa_6.fit_transform(vtrain)\n",
    "\n",
    "score_6 = []\n",
    "for i in range(1,21):\n",
    "    kmeans6 = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "    kmeans6.fit(vtrain_lsa6)\n",
    "    y_hat_kmeans6 = kmeans6.predict(vtrain_lsa6)\n",
    "    centers = kmeans6.cluster_centers_\n",
    "#     print('The KMeans score for the truncated, 6 component data, using',i,'clusters is:',\n",
    "#           kmeans5.score(vtrain_lsa6[y_hat_kmeans6]))\n",
    "    score_6.append(kmeans6.score(vtrain_lsa6[y_hat_kmeans6]))\n",
    "print(score_6)\n",
    "\n",
    "# # The plot below wasn't doing much for me\n",
    "# fig = plt.figure(0,figsize=(32,32))\n",
    "# ax = fig.add_subplot(4,4,2)\n",
    "\n",
    "# # Draw the plots\n",
    "# ax.scatter(vtrain_lsa2[:, 0], vtrain_lsa2[:, 1], s=1, c=y_hat_kmeans)\n",
    "# ax.scatter(centers[:, 0], centers[:, 1], c='red', s=50, alpha=0.6)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.13235994, -2.3239353 , -1.9670322 , -2.13366984, -2.16196193,\n",
       "       -2.08809671, -2.09155992, -2.32646581, -2.10368873, -2.08441638,\n",
       "       -1.94689032, -2.29990774, -2.05141319, -2.3432188 , -2.34865185,\n",
       "       -2.21702992, -2.19896781])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model using the raw data fit the train data, but wouldn't predict the test data, as it wasn't the right shape. \n",
    "# I think this goes back to the different dimensionality between the two data sets. \n",
    "# vtraind = vtrain.toarray()\n",
    "# vtestd = vtest.toarray()\n",
    "# gmm = GaussianMixture()\n",
    "# gmm.fit(vtraind)\n",
    "# gmm.predict(vtestd)\n",
    "\n",
    "svd_2 = TruncatedSVD(n_components = 2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_2 = make_pipeline(svd_2, normalizer)\n",
    "vtrain_lsa2 = lsa_2.fit_transform(vtrain)\n",
    "\n",
    "svd_2_test = TruncatedSVD(n_components = 2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_2_test = make_pipeline(svd_2_test, normalizer)\n",
    "vtest_lsa2 = lsa_2_test.fit_transform(vtest)\n",
    "\n",
    "gmm2 = GaussianMixture()\n",
    "gmm2.fit(vtrain_lsa2)\n",
    "gmm2.score_samples(vtest_lsa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the train data with positive labels, fit the GMM model\n",
    "p2pos = p2_train[train_labels == 1]\n",
    "pos = GaussianMixture(n_components=4, covariance_type='full')\n",
    "pos.fit(p2pos)\n",
    "\n",
    "# Separate out the train data with negative labels, fit the GMM model    \n",
    "p2neg = p2_train[train_labels != 1]    \n",
    "neg = GaussianMixture(n_components=4, covariance_type='full')\n",
    "neg.fit(p2neg)\n",
    "\n",
    "# Fit the test data to both models\n",
    "wlprob_pos = pos.score_samples(p2_test)\n",
    "wlprob_neg = neg.score_samples(p2_test)\n",
    "\n",
    "# Create an array that picks the winner from each prediction \n",
    "pred_test_labels = np.array(wlprob_pos > wlprob_neg).astype(int)\n",
    "\n",
    "# Calculate the accuracy to 6 decimal places, print it out\n",
    "accuracy = np.round(np.mean(pred_test_labels == test_labels), 6)   \n",
    "print('The accuracy of this model is:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
