{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Acts of Pizza - Baseline Model & EDA\n",
    "## Authors: Ben Arnoldy, Mary Boardman, Zach Merritt, and Kevin Gifford\n",
    "#### Kaggle Competition Description:\n",
    "In machine learning, it is often said there are no free lunches. How wrong we were.\n",
    "\n",
    "This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n",
    "\n",
    "\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\" says one hopeful poster. What about making an algorithm?\n",
    "\n",
    "Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:\n",
    "\n",
    "Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.\n",
    "_______________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Title: EDA & Model Baseline\n",
    "#### Purpose: Load the 'Random Acts of Pizza' train and test data. Conduct an exploratory data analysis to gain an understanding of the data. Create a baseline Logisitic Regression model using non-text (numeric) fields only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load Data and Modules, Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load Data and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from subprocess import check_output\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (4040, 32)\n",
      "Test Shape: (1631, 17)\n"
     ]
    }
   ],
   "source": [
    "#1. Train Data\n",
    "with open('../data/train.json') as fin:\n",
    "    trainjson = json.load(fin)\n",
    "train = pd.io.json.json_normalize(trainjson)\n",
    "#2. Test Data\n",
    "with open('../data/test.json') as fin:\n",
    "    testjson = json.load(fin)\n",
    "test = pd.io.json.json_normalize(testjson)\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Find any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "giver_username_if_known                                    0\n",
       "number_of_downvotes_of_request_at_retrieval                0\n",
       "number_of_upvotes_of_request_at_retrieval                  0\n",
       "post_was_edited                                            0\n",
       "request_id                                                 0\n",
       "request_number_of_comments_at_retrieval                    0\n",
       "request_text                                               0\n",
       "request_text_edit_aware                                    0\n",
       "request_title                                              0\n",
       "requester_account_age_in_days_at_request                   0\n",
       "requester_account_age_in_days_at_retrieval                 0\n",
       "requester_days_since_first_post_on_raop_at_request         0\n",
       "requester_days_since_first_post_on_raop_at_retrieval       0\n",
       "requester_number_of_comments_at_request                    0\n",
       "requester_number_of_comments_at_retrieval                  0\n",
       "requester_number_of_comments_in_raop_at_request            0\n",
       "requester_number_of_comments_in_raop_at_retrieval          0\n",
       "requester_number_of_posts_at_request                       0\n",
       "requester_number_of_posts_at_retrieval                     0\n",
       "requester_number_of_posts_on_raop_at_request               0\n",
       "requester_number_of_posts_on_raop_at_retrieval             0\n",
       "requester_number_of_subreddits_at_request                  0\n",
       "requester_received_pizza                                   0\n",
       "requester_subreddits_at_request                            0\n",
       "requester_upvotes_minus_downvotes_at_request               0\n",
       "requester_upvotes_minus_downvotes_at_retrieval             0\n",
       "requester_upvotes_plus_downvotes_at_request                0\n",
       "requester_upvotes_plus_downvotes_at_retrieval              0\n",
       "requester_user_flair                                    3046\n",
       "requester_username                                         0\n",
       "unix_timestamp_of_request                                  0\n",
       "unix_timestamp_of_request_utc                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data except in column \"requester_user_flair.\" We see in the next section that this isn't a column in the test data, so we may just elect to not use it to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Identify common columns between test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns in train and test:\n",
      "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
      "       'request_title', 'requester_account_age_in_days_at_request',\n",
      "       'requester_days_since_first_post_on_raop_at_request',\n",
      "       'requester_number_of_comments_at_request',\n",
      "       'requester_number_of_comments_in_raop_at_request',\n",
      "       'requester_number_of_posts_at_request',\n",
      "       'requester_number_of_posts_on_raop_at_request',\n",
      "       'requester_number_of_subreddits_at_request',\n",
      "       'requester_subreddits_at_request',\n",
      "       'requester_upvotes_minus_downvotes_at_request',\n",
      "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
      "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'],\n",
      "      dtype='object')\n",
      "----\n",
      "Columns in train but NOT test:\n",
      "Index(['number_of_downvotes_of_request_at_retrieval',\n",
      "       'number_of_upvotes_of_request_at_retrieval', 'post_was_edited',\n",
      "       'request_number_of_comments_at_retrieval', 'request_text',\n",
      "       'requester_account_age_in_days_at_retrieval',\n",
      "       'requester_days_since_first_post_on_raop_at_retrieval',\n",
      "       'requester_number_of_comments_at_retrieval',\n",
      "       'requester_number_of_comments_in_raop_at_retrieval',\n",
      "       'requester_number_of_posts_at_retrieval',\n",
      "       'requester_number_of_posts_on_raop_at_retrieval',\n",
      "       'requester_received_pizza',\n",
      "       'requester_upvotes_minus_downvotes_at_retrieval',\n",
      "       'requester_upvotes_plus_downvotes_at_retrieval',\n",
      "       'requester_user_flair'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Common columns in train and test:\")\n",
    "print(train.columns[train.columns.isin(test.columns)])\n",
    "print(\"----\")\n",
    "print(\"Columns in train but NOT test:\")\n",
    "print(train.columns[~train.columns.isin(test.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, there is a series of columns in the training data only. These columns reflect data about the post (e.g., the #of upvotes) at the time this Reddit data was retrieved. We use certain supervised and unsupversied techniques to derive value from this data (even though that information is not provided on the data set we will be predicting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Create training data, labels, and special 'in training only' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_master = train[['requester_received_pizza']]\n",
    "train_data_master = train[test.columns & train.columns]\n",
    "train_only_data_master = train[train.columns[~train.columns.isin(test.columns)]].drop(['requester_received_pizza'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4040, 32) (4040, 17)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, train_data_master.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Set column types and profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_master = train_data_master.assign(\n",
    "    unix_timestamp_of_request = pd.to_datetime(\n",
    "        train_data_master.unix_timestamp_of_request, unit = \"s\"),\n",
    "    unix_timestamp_of_request_utc = pd.to_datetime(\n",
    "        train_data_master.unix_timestamp_of_request_utc, unit = \"s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4040 entries, 0 to 4039\n",
      "Data columns (total 17 columns):\n",
      "giver_username_if_known                               4040 non-null object\n",
      "request_id                                            4040 non-null object\n",
      "request_text_edit_aware                               4040 non-null object\n",
      "request_title                                         4040 non-null object\n",
      "requester_account_age_in_days_at_request              4040 non-null float64\n",
      "requester_days_since_first_post_on_raop_at_request    4040 non-null float64\n",
      "requester_number_of_comments_at_request               4040 non-null int64\n",
      "requester_number_of_comments_in_raop_at_request       4040 non-null int64\n",
      "requester_number_of_posts_at_request                  4040 non-null int64\n",
      "requester_number_of_posts_on_raop_at_request          4040 non-null int64\n",
      "requester_number_of_subreddits_at_request             4040 non-null int64\n",
      "requester_subreddits_at_request                       4040 non-null object\n",
      "requester_upvotes_minus_downvotes_at_request          4040 non-null int64\n",
      "requester_upvotes_plus_downvotes_at_request           4040 non-null int64\n",
      "requester_username                                    4040 non-null object\n",
      "unix_timestamp_of_request                             4040 non-null datetime64[ns]\n",
      "unix_timestamp_of_request_utc                         4040 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(7), object(6)\n",
      "memory usage: 536.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data_master.describe()\n",
    "train_data_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe we cannot just vectorize the train data because the text-based data\n",
    "# is held within one specific feature called request_test_edit_aware.\n",
    "# Below I repurposed some of Zach's code to vectorize\n",
    "\n",
    "#Create Sparse matrix of words\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#Split train and test data\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "    train_data_master['request_text_edit_aware'], \n",
    "    train_labels_master.values.ravel(), test_size=0.29, random_state=0)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#    train_data_master,\n",
    "#    train_labels_master.values.ravel(), test_size=0.29, random_state=0)\n",
    "x_train = count_vect.fit_transform(x_train_text)\n",
    "x_test = count_vect.transform(x_test_text)\n",
    "cv_feature_names = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SVD (in lieu of PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25966847]\n",
      "[ 0.25966847  0.02954686]\n",
      "[ 0.25966847  0.02954676  0.02266197]\n",
      "[ 0.25966847  0.02954691  0.02266269  0.02118754]\n",
      "[ 0.25966847  0.02954692  0.02266293  0.02118774  0.01776063]\n",
      "[ 0.25966847  0.0295469   0.02266255  0.02118853  0.01776144  0.01537296]\n",
      "[ 0.25966847  0.02954693  0.02266229  0.02118824  0.01776452  0.01537306\n",
      "  0.01309128]\n",
      "[ 0.25966847  0.02954693  0.02266291  0.02118755  0.01776222  0.01537325\n",
      "  0.01309176  0.01216625]\n",
      "[ 0.25966847  0.02954692  0.02266274  0.02118772  0.01776315  0.01537217\n",
      "  0.0130813   0.01218914  0.0110454 ]\n",
      "[ 0.25966847  0.02954693  0.0226628   0.02118786  0.01776331  0.01537374\n",
      "  0.01309167  0.01218854  0.01104436  0.01060167]\n",
      "[ 0.25966847  0.02954693  0.02266268  0.02118768  0.01776264  0.01537374\n",
      "  0.01309247  0.0121846   0.01104337  0.01060253  0.0101695 ]\n",
      "[ 0.25966847  0.02954693  0.02266252  0.0211875   0.01776295  0.01537366\n",
      "  0.01309259  0.0121883   0.01103768  0.0106001   0.01016479  0.00949482]\n",
      "[ 0.25966847  0.02954692  0.02266265  0.02118766  0.01776296  0.01537367\n",
      "  0.01309204  0.01218513  0.01104799  0.0105806   0.01016345  0.00946381\n",
      "  0.0090248 ]\n",
      "[ 0.25966847  0.02954693  0.02266264  0.0211876   0.01776275  0.01537374\n",
      "  0.01309221  0.01218904  0.01104868  0.01060037  0.01017196  0.00949651\n",
      "  0.0090289   0.00881806]\n"
     ]
    }
   ],
   "source": [
    "# Since PCA does not work for a sparse matrix, we are using Truncated SVD instead for dimensionality reduction\n",
    "for top_comps in range (1, 15):\n",
    "    svd = TruncatedSVD(n_components = top_comps)\n",
    "    svd.fit(x_train)\n",
    "    print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of vectorizer shows a major dropoff in explained variance after the 1st component\n",
    "# This suggests that projecting down to 1 component might be best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try various hyperparameters with GMM on text field, try to classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for 1 SVD comps, 1 GMM comps, Covariance type spherical = 0.652730375427\n",
      "Accuracy score for 1 SVD comps, 2 GMM comps, Covariance type spherical = 0.581911262799\n",
      "Accuracy score for 1 SVD comps, 3 GMM comps, Covariance type spherical = 0.570819112628\n",
      "Accuracy score for 1 SVD comps, 4 GMM comps, Covariance type spherical = 0.523890784983\n",
      "Accuracy score for 1 SVD comps, 5 GMM comps, Covariance type spherical = 0.584470989761\n",
      "Accuracy score for 1 SVD comps, 6 GMM comps, Covariance type spherical = 0.575938566553\n",
      "Accuracy score for 1 SVD comps, 7 GMM comps, Covariance type spherical = 0.587030716724\n",
      "Accuracy score for 1 SVD comps, 8 GMM comps, Covariance type spherical = 0.555460750853\n",
      "Accuracy score for 1 SVD comps, 9 GMM comps, Covariance type spherical = 0.574232081911\n",
      "Accuracy score for 1 SVD comps, 10 GMM comps, Covariance type spherical = 0.579351535836\n",
      "Accuracy score for 1 SVD comps, 11 GMM comps, Covariance type spherical = 0.578498293515\n",
      "Accuracy score for 1 SVD comps, 12 GMM comps, Covariance type spherical = 0.553754266212\n",
      "Accuracy score for 2 SVD comps, 1 GMM comps, Covariance type spherical = 0.65614334471\n",
      "Accuracy score for 2 SVD comps, 2 GMM comps, Covariance type spherical = 0.557167235495\n",
      "Accuracy score for 2 SVD comps, 3 GMM comps, Covariance type spherical = 0.545221843003\n",
      "Accuracy score for 2 SVD comps, 4 GMM comps, Covariance type spherical = 0.527303754266\n",
      "Accuracy score for 2 SVD comps, 5 GMM comps, Covariance type spherical = 0.529863481229\n",
      "Accuracy score for 2 SVD comps, 6 GMM comps, Covariance type spherical = 0.528156996587\n",
      "Accuracy score for 3 SVD comps, 1 GMM comps, Covariance type spherical = 0.662116040956\n",
      "Accuracy score for 3 SVD comps, 2 GMM comps, Covariance type spherical = 0.548634812287\n",
      "Accuracy score for 3 SVD comps, 3 GMM comps, Covariance type spherical = 0.551194539249\n",
      "Accuracy score for 3 SVD comps, 4 GMM comps, Covariance type spherical = 0.524744027304\n",
      "Accuracy score for 4 SVD comps, 1 GMM comps, Covariance type spherical = 0.662116040956\n",
      "Accuracy score for 4 SVD comps, 2 GMM comps, Covariance type spherical = 0.546928327645\n",
      "Accuracy score for 4 SVD comps, 3 GMM comps, Covariance type spherical = 0.532423208191\n",
      "Accuracy score for 5 SVD comps, 1 GMM comps, Covariance type spherical = 0.658703071672\n",
      "Accuracy score for 5 SVD comps, 2 GMM comps, Covariance type spherical = 0.530716723549\n",
      "Accuracy score for 6 SVD comps, 1 GMM comps, Covariance type spherical = 0.665529010239\n",
      "Accuracy score for 6 SVD comps, 2 GMM comps, Covariance type spherical = 0.525597269625\n",
      "Accuracy score for 7 SVD comps, 1 GMM comps, Covariance type spherical = 0.664675767918\n",
      "Accuracy score for 8 SVD comps, 1 GMM comps, Covariance type spherical = 0.662116040956\n",
      "Accuracy score for 9 SVD comps, 1 GMM comps, Covariance type spherical = 0.664675767918\n",
      "Accuracy score for 10 SVD comps, 1 GMM comps, Covariance type spherical = 0.667235494881\n",
      "Accuracy score for 11 SVD comps, 1 GMM comps, Covariance type spherical = 0.66638225256\n",
      "Accuracy score for 12 SVD comps, 1 GMM comps, Covariance type spherical = 0.66638225256\n",
      "Accuracy score for 1 SVD comps, 1 GMM comps, Covariance type diag = 0.652730375427\n",
      "Accuracy score for 1 SVD comps, 2 GMM comps, Covariance type diag = 0.581911262799\n",
      "Accuracy score for 1 SVD comps, 3 GMM comps, Covariance type diag = 0.589590443686\n",
      "Accuracy score for 1 SVD comps, 4 GMM comps, Covariance type diag = 0.529863481229\n",
      "Accuracy score for 1 SVD comps, 5 GMM comps, Covariance type diag = 0.596416382253\n",
      "Accuracy score for 1 SVD comps, 6 GMM comps, Covariance type diag = 0.576791808874\n",
      "Accuracy score for 1 SVD comps, 7 GMM comps, Covariance type diag = 0.556313993174\n",
      "Accuracy score for 1 SVD comps, 8 GMM comps, Covariance type diag = 0.587030716724\n",
      "Accuracy score for 1 SVD comps, 9 GMM comps, Covariance type diag = 0.559726962457\n",
      "Accuracy score for 1 SVD comps, 10 GMM comps, Covariance type diag = 0.540102389078\n",
      "Accuracy score for 1 SVD comps, 11 GMM comps, Covariance type diag = 0.571672354949\n",
      "Accuracy score for 1 SVD comps, 12 GMM comps, Covariance type diag = 0.582764505119\n",
      "Accuracy score for 2 SVD comps, 1 GMM comps, Covariance type diag = 0.663822525597\n",
      "Accuracy score for 2 SVD comps, 2 GMM comps, Covariance type diag = 0.554607508532\n",
      "Accuracy score for 2 SVD comps, 3 GMM comps, Covariance type diag = 0.540955631399\n",
      "Accuracy score for 2 SVD comps, 4 GMM comps, Covariance type diag = 0.523890784983\n",
      "Accuracy score for 2 SVD comps, 5 GMM comps, Covariance type diag = 0.546928327645\n",
      "Accuracy score for 2 SVD comps, 6 GMM comps, Covariance type diag = 0.53156996587\n",
      "Accuracy score for 3 SVD comps, 1 GMM comps, Covariance type diag = 0.667235494881\n",
      "Accuracy score for 3 SVD comps, 2 GMM comps, Covariance type diag = 0.544368600683\n",
      "Accuracy score for 3 SVD comps, 3 GMM comps, Covariance type diag = 0.543515358362\n",
      "Accuracy score for 3 SVD comps, 4 GMM comps, Covariance type diag = 0.569112627986\n",
      "Accuracy score for 4 SVD comps, 1 GMM comps, Covariance type diag = 0.669795221843\n",
      "Accuracy score for 4 SVD comps, 2 GMM comps, Covariance type diag = 0.557167235495\n",
      "Accuracy score for 4 SVD comps, 3 GMM comps, Covariance type diag = 0.543515358362\n",
      "Accuracy score for 5 SVD comps, 1 GMM comps, Covariance type diag = 0.668088737201\n",
      "Accuracy score for 5 SVD comps, 2 GMM comps, Covariance type diag = 0.548634812287\n",
      "Accuracy score for 6 SVD comps, 1 GMM comps, Covariance type diag = 0.668941979522\n",
      "Accuracy score for 6 SVD comps, 2 GMM comps, Covariance type diag = 0.535836177474\n",
      "Accuracy score for 7 SVD comps, 1 GMM comps, Covariance type diag = 0.663822525597\n",
      "Accuracy score for 8 SVD comps, 1 GMM comps, Covariance type diag = 0.667235494881\n",
      "Accuracy score for 9 SVD comps, 1 GMM comps, Covariance type diag = 0.660409556314\n",
      "Accuracy score for 10 SVD comps, 1 GMM comps, Covariance type diag = 0.659556313993\n",
      "Accuracy score for 11 SVD comps, 1 GMM comps, Covariance type diag = 0.662116040956\n",
      "Accuracy score for 12 SVD comps, 1 GMM comps, Covariance type diag = 0.660409556314\n",
      "Accuracy score for 1 SVD comps, 1 GMM comps, Covariance type tied = 0.652730375427\n",
      "Accuracy score for 1 SVD comps, 2 GMM comps, Covariance type tied = 0.616894197952\n",
      "Accuracy score for 1 SVD comps, 3 GMM comps, Covariance type tied = 0.582764505119\n",
      "Accuracy score for 1 SVD comps, 4 GMM comps, Covariance type tied = 0.630546075085\n",
      "Accuracy score for 1 SVD comps, 5 GMM comps, Covariance type tied = 0.576791808874\n",
      "Accuracy score for 1 SVD comps, 6 GMM comps, Covariance type tied = 0.564846416382\n",
      "Accuracy score for 1 SVD comps, 7 GMM comps, Covariance type tied = 0.609215017065\n",
      "Accuracy score for 1 SVD comps, 8 GMM comps, Covariance type tied = 0.57252559727\n",
      "Accuracy score for 1 SVD comps, 9 GMM comps, Covariance type tied = 0.565699658703\n",
      "Accuracy score for 1 SVD comps, 10 GMM comps, Covariance type tied = 0.588737201365\n",
      "Accuracy score for 1 SVD comps, 11 GMM comps, Covariance type tied = 0.560580204778\n",
      "Accuracy score for 1 SVD comps, 12 GMM comps, Covariance type tied = 0.561433447099\n",
      "Accuracy score for 2 SVD comps, 1 GMM comps, Covariance type tied = 0.662969283276\n",
      "Accuracy score for 2 SVD comps, 2 GMM comps, Covariance type tied = 0.648464163823\n",
      "Accuracy score for 2 SVD comps, 3 GMM comps, Covariance type tied = 0.636518771331\n",
      "Accuracy score for 2 SVD comps, 4 GMM comps, Covariance type tied = 0.633105802048\n",
      "Accuracy score for 2 SVD comps, 5 GMM comps, Covariance type tied = 0.584470989761\n",
      "Accuracy score for 2 SVD comps, 6 GMM comps, Covariance type tied = 0.606655290102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a992cdea3ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mgmm_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-a992cdea3ab7>\u001b[0m in \u001b[0;36mgmm_trials\u001b[0;34m(sparse, max_params)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtop_accuracy_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtop_accuracy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a992cdea3ab7>\u001b[0m in \u001b[0;36mtrain_gmm\u001b[0;34m(svd_comps, gmm_comps, cov_type, sparse)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_test_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpos_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm_fit_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_test_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mneg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm_fit_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_test_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# make pick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos_score\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mLog\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mGaussian\u001b[0m \u001b[0mmixture\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36mscore_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_weighted_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36m_estimate_weighted_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mweighted_log_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_component\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_estimate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         return _estimate_log_gaussian_prob(\n\u001b[0;32m--> 675\u001b[0;31m             X, self.means_, self.precisions_cholesky_, self.covariance_type)\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_estimate_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_log_gaussian_prob\u001b[0;34m(X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions_chol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions_chol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mlog_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcovariance_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'diag'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1834\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_gmm(svd_comps = 2, gmm_comps = 4, cov_type = 'full', sparse = True):\n",
    "    \n",
    "    # project data to few dimensions using SVD if sparse, PCA if dense\n",
    "    if (sparse): svd_test = TruncatedSVD(n_components=svd_comps)\n",
    "    else: svd_test = PCA(n_components=svd_comps)\n",
    "    svd_test_ft = svd_test.fit_transform(x_test)\n",
    "    if (sparse): svd_train = TruncatedSVD(n_components=svd_comps)\n",
    "    else: svd_train = PCA(n_components=svd_comps)\n",
    "    svd_train_ft = svd_train.fit_transform(x_train)\n",
    "\n",
    "    # create two sets of svd data, one that's positively labeled, one that's negatively labeled\n",
    "    pos_svd = svd_train_ft[y_train == 1]\n",
    "    neg_svd = svd_train_ft[y_train == 0]\n",
    "    \n",
    "    # fit a GMM for pos and neg datasets\n",
    "    gmm_pos = GaussianMixture(n_components=gmm_comps, covariance_type=cov_type) \n",
    "    gmm_fit_pos = gmm_pos.fit(pos_svd)\n",
    "    gmm_neg = GaussianMixture(n_components=gmm_comps, covariance_type=cov_type) \n",
    "    gmm_fit_neg = gmm_neg.fit(neg_svd)\n",
    "\n",
    "    prediction = np.ndarray(shape=y_test.shape)\n",
    "    \n",
    "    for sample in range(svd_test_ft.shape[0]):\n",
    "        pos_score = gmm_fit_pos.score(svd_test_ft[sample].reshape(1,-1))\n",
    "        neg_score = gmm_fit_neg.score(svd_test_ft[sample].reshape(1,-1))\n",
    "        # make pick\n",
    "        if (pos_score >= neg_score): prediction[sample] = 1\n",
    "        else: prediction[sample] = 0 \n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(y_test, prediction)\n",
    "    # f1 = metrics.f1_score(y_test, prediction)\n",
    "    print(\"Accuracy score for\", svd_comps,\"SVD comps,\",gmm_comps,\"GMM comps,\",\"Covariance type\",cov_type,\"=\",accuracy)\n",
    "\n",
    "    return(accuracy)\n",
    "\n",
    "def gmm_trials(sparse = True, max_params = 50):\n",
    "\n",
    "    def calc_limits(max_params = 50):\n",
    "        # this function helps estaish the svd and gmm settings to stay within max_params\n",
    "\n",
    "        valid_configs = [] # list of tuples (num_svd_components, num_gmm_components)\n",
    "\n",
    "        for cov_type in ['spherical', 'diag', 'tied', 'full']:\n",
    "            for svd_comp in range(1,20): \n",
    "                for gmm_comp in range(1,20): \n",
    "                    if ((svd_comp + svd_comp) * gmm_comp) * 2 <= max_params:\n",
    "                        valid_configs.append((svd_comp, gmm_comp, cov_type))\n",
    "\n",
    "        return(valid_configs)\n",
    "\n",
    "    configs = calc_limits(max_params) # get the valid configurations\n",
    "\n",
    "    top_accuracy_val = 0 # keeps track of top accuracy value\n",
    "    top_accuracy_config = () # keeps track of config that creates top accuracy value\n",
    "    \n",
    "    for config in configs:\n",
    "        accuracy = train_gmm(config[0], config[1], config[2])\n",
    "        if accuracy > top_accuracy_val:\n",
    "            top_accuracy_val = accuracy\n",
    "            top_accuracy_config = config\n",
    "\n",
    "    print(\"*********************************************************************************\")\n",
    "    print(\"The best accuracy score is\", top_accuracy_val)\n",
    "    print(\"To get it, set SVD comps to\", top_accuracy_config[0], \", GMM comps to\", top_accuracy_config[1],\", and covariance type to\", top_accuracy_config[2])\n",
    "\n",
    "    \n",
    "#Create Sparse matrix of words\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#Split train and test data\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "    train_data_master['request_text_edit_aware'], \n",
    "    train_labels_master.values.ravel(), test_size=0.29, random_state=0)\n",
    "x_train = count_vect.fit_transform(x_train_text)\n",
    "x_test = count_vect.transform(x_test_text)\n",
    "\n",
    "gmm_trials(sparse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: The best accuracy score doesn't beat the 0.68 naive bayes baseline, so dimensionality reduction on the vectorized text field doesn't help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: Dimensionality reduction on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using ALL features, even those not in the test data. \n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler_allfeats = min_max_scaler.fit_transform(\n",
    "    train.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "mn_mx_scaler_commonfeats = min_max_scaler.fit_transform(\n",
    "    train_data_master.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test = mn_mx_scaler_allfeats[:3232], mn_mx_scaler_commonfeats[3232:] \n",
    "y_train, y_test = train_labels_master[:3232].values.ravel(), train_labels_master[3232:].values.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_trials(sparse = False, max_params = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: A 0.74 accuracy level is less than what was achieved with logistic regression baseline of 0.75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using just the features in common with train and test. \n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler = min_max_scaler.fit_transform(\n",
    "    train.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test = mn_mx_scaler[:3232], mn_mx_scaler[3232:] \n",
    "y_train, y_test = train_labels_master[:3232].values.ravel(), train_labels_master[3232:].values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_trials(sparse = False, max_params = 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: a 0.73 accuracy score is less than the logistic regression baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce vectorized word field, plug it into logistic regression with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Sparse matrix of words\n",
    "count_vect = CountVectorizer()\n",
    "x_train = train_data_master[:3232]['request_text_edit_aware']\n",
    "x_test = train_data_master[3232:]['request_text_edit_aware'] \n",
    "train_features = count_vect.fit_transform(x_train)\n",
    "test_features = count_vect.transform(x_test)\n",
    "\n",
    "#Reduce the vectorized word feature to small number of numerical components\n",
    "svd_comps = 110 # 110 after some trial and error, this was the best\n",
    "svd = TruncatedSVD(n_components=svd_comps)\n",
    "svd_ft_tr = svd.fit_transform(train_features)\n",
    "svd_ft_dv = svd.fit_transform(test_features)\n",
    "svd_ft = np.concatenate((svd_ft_tr, svd_ft_dv), axis=0)\n",
    "\n",
    "# add the newly reduced text field feature to the train and test data\n",
    "train_data_SVD = train_data_master.copy(deep = True)\n",
    "for comp in range(svd_comps):\n",
    "    train_data_SVD['SVD'+str(comp)] = svd_ft[:,comp]\n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler = min_max_scaler.fit_transform(train_data_SVD.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test = mn_mx_scaler[:3232], mn_mx_scaler[3232:] \n",
    "y_train, y_test = train_labels_master[:3232].values.ravel(), train_labels_master[3232:].values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do logistic regression on our new train data\n",
    "\n",
    "def model_report(title, y_test, predictions):\n",
    "\n",
    "    \"\"\"\n",
    "    Output: Classification report, confusion matrix, and ROC curve\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    print(\"---------\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    all_sample_title = 'Accuracy: {0}'.format(round(metrics.accuracy_score(y_test, predictions),2))\n",
    "    plt.title(all_sample_title, size = 15)\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "#Train Model\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "#Output model report\n",
    "model_report(\"Logistic Regression (using only common numeric fields)\",y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a9d75cb65d85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvd_comps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msvd_ft_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msvd_ft_dv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msvd_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_ft_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_ft_dv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_features' is not defined"
     ]
    }
   ],
   "source": [
    "#Create Sparse matrix of words\n",
    "count_vect = TfidfVectorizer()\n",
    "x_train = train_data_master[:3232]['request_text_edit_aware']\n",
    "x_test = train_data_master[3232:]['request_text_edit_aware'] \n",
    "train_features = count_vect.fit_transform(x_train)\n",
    "test_features = count_vect.transform(x_test)\n",
    "\n",
    "#Reduce the vectorized word feature to small number of numerical components\n",
    "svd_comps = 25 # 25 after some trial and error, this was the best\n",
    "svd = TruncatedSVD(n_components=svd_comps)\n",
    "svd_ft_tr = svd.fit_transform(train_features)\n",
    "svd_ft_dv = svd.fit_transform(dev_features)\n",
    "svd_ft = np.concatenate((svd_ft_tr, svd_ft_dv), axis=0)\n",
    "\n",
    "# add the newly reduced text field feature to the train and test data\n",
    "train_data_SVD = train_data_master.copy(deep = True)\n",
    "for comp in range(svd_comps):\n",
    "    train_data_SVD['SVD'+str(comp)] = svd_ft[:,comp]\n",
    "\n",
    "#Normalize all fields (numeric)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mn_mx_scaler = min_max_scaler.fit_transform(train_data_SVD.select_dtypes(include = ['float64', 'int64','datetime64[ns]']).apply(pd.to_numeric).values)\n",
    "\n",
    "#Split train and test data\n",
    "x_train, x_test = mn_mx_scaler[:3232], mn_mx_scaler[3232:] \n",
    "y_train, y_test = train_labels_master[:3232].values.ravel(), train_labels_master[3232:].values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"We're completely broke and craving pizza. I thought I was super smart and ordered Domino's gift cards on Walmart.com with my Paypal - they aren't going to be e-delivered until tomorrow! Payday is Friday, and we don't have any money for groceries until then... but I will reciprocate! I will also send a homemade candle if someone wants... if you're kind enough to feed hubs and I, let me know if you'd like one, and I'll let you know the scents that we have available. \\n\\nThank you!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-78022a4f4023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# all parameters not specified are set to their defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mlogisticRegr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mlogisticRegr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticRegr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"We're completely broke and craving pizza. I thought I was super smart and ordered Domino's gift cards on Walmart.com with my Paypal - they aren't going to be e-delivered until tomorrow! Payday is Friday, and we don't have any money for groceries until then... but I will reciprocate! I will also send a homemade candle if someone wants... if you're kind enough to feed hubs and I, let me know if you'd like one, and I'll let you know the scents that we have available. \\n\\nThank you!\""
     ]
    }
   ],
   "source": [
    "# do logistic regression on our new train data\n",
    "\n",
    "def model_report(title, y_test, predictions):\n",
    "\n",
    "    \"\"\"\n",
    "    Output: Classification report, confusion matrix, and ROC curve\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    print(\"---------\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    all_sample_title = 'Accuracy: {0}'.format(round(metrics.accuracy_score(y_test, predictions),2))\n",
    "    plt.title(all_sample_title, size = 15)\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "#Train Model\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression(C=.01)\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "#Output model report\n",
    "model_report(\"Logistic Regression (using only common numeric fields)\",y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* Reducing the vectorized word feature got us to 0.68 accuracy - the exact same accuracy that Naive Bayes got us. \n",
    "* Reducing all the features (even those not in the test set) got us to 0.74 accuracy - a bit less than the 0.75 accuracy from logistic regression on the features. \n",
    "* Reducing just the features that are shared by train and test sets got us to 0.73 accuracy - even further from the 0.75 accuracy of logistic regression.\n",
    "* Vectorizing the text field, reducing it through SVD to 110 components, adding it to the train data, and running logistic regression again did improve accuracy by small margins.\n",
    "* Tfidf vectorizing the text field before doing the above did not improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
