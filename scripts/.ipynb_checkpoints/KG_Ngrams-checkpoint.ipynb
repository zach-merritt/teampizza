{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from subprocess import check_output\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (4040, 32)\n",
      "Test Shape: (1631, 17)\n",
      "(3232, 17)\n",
      "(646, 17)\n",
      "(162, 17)\n",
      "(3232,)\n",
      "(646,)\n",
      "(162,)\n"
     ]
    }
   ],
   "source": [
    "#1. Train Data\n",
    "with open('../data/train.json') as fin:\n",
    "   trainjson = json.load(fin)\n",
    "train = pd.io.json.json_normalize(trainjson)\n",
    "#2. Test Data\n",
    "with open('../data/test.json') as fin:\n",
    "   testjson = json.load(fin)\n",
    "test = pd.io.json.json_normalize(testjson)\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)\n",
    "\n",
    "train_labels_master = train[['requester_received_pizza']]\n",
    "train_data_master = train[test.columns & train.columns]\n",
    "train_only_data_master = train[train.columns[~train.columns.isin(test.columns)]].drop(['requester_received_pizza'], axis = 1)\n",
    "\n",
    "#Apply train_test_split twice to get train, test, and dev set\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "   train_data_master,\n",
    "   train_labels_master.values.ravel(), test_size=0.2, random_state=0)\n",
    "\n",
    "x_test, x_dev, y_test, y_dev = train_test_split(\n",
    "   x_test,\n",
    "   y_test, test_size=0.2, random_state=0)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_dev.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized score for BernoulliNB (alpha=10.0): 0.7561881188118812\n",
      "\n",
      "0.738390092879\n",
      "0.738390092879\n"
     ]
    }
   ],
   "source": [
    "# Isolate the text column for the training and dev dataframes\n",
    "import re\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "x_train_text = x_train.request_text_edit_aware\n",
    "x_test_text = x_test.request_text_edit_aware\n",
    "\n",
    "### Run text cleaning preprocessing on the training dataset to remove a lot of the junk.\n",
    "\n",
    "# Define custom text preprocessor\n",
    "def text_cleaner(s):\n",
    "    # Establish a compiled regex that finds words shorter than 3 characters\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    text = s.lower()\n",
    "    \n",
    "    # Remove newlines and punctuation marks\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub('[,?]',' ',text)\n",
    "    text = re.sub('\\. ',' ',text)\n",
    "    text = re.sub(' \\.',' ',text)\n",
    "    text = re.sub('\\.{2,}',' ',text)\n",
    "    text = re.sub(r'/',' ',text)\n",
    "    text = re.sub('-','',text)\n",
    "    text = re.sub('\"','',text)\n",
    "    text = re.sub('[<>()]',' ',text)\n",
    "\n",
    "    # Convert sequences of numbers to zero\n",
    "    text = re.sub('\\d+', '0', text)\n",
    "    \n",
    "    # Remove short words\n",
    "    text = shortword.sub('', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(' +',' ',text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Set up count vectorizer to use custom preprocessor\n",
    "# Using bigrams in the vectorizer gains about a percentage point of accuracy, but it appears that using\n",
    "# trigrams or larger n-grams doesn't provide any further gains. \n",
    "vectotron = CountVectorizer(preprocessor=text_cleaner, analyzer='word',ngram_range=(2,2)) \n",
    "x_train_vect = vectotron.fit_transform(x_train_text)\n",
    "x_test_vect = vectotron.transform(x_test_text)\n",
    "# print(vectotron.vocabulary_)\n",
    "\n",
    "\n",
    "# Fit a Bernoulli Naive Bayes model using the vectorized text and use GridSearch to optimize params\n",
    "model_TextNB = BernoulliNB()\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 50.0, 100.0]}\n",
    "BernNB_clf = GridSearchCV(model_TextNB,param_grid=alphas)\n",
    "BernNB_clf.fit(x_train_vect,y_train)\n",
    "print('Optimized score for BernoulliNB (alpha=',BernNB_clf.best_params_['alpha'],'): ',BernNB_clf.best_score_,'\\n',sep='')\n",
    "alpha_optimal = BernNB_clf.best_params_['alpha']\n",
    "\n",
    "# Predict and check accuracy\n",
    "model_TextNB = BernoulliNB(alpha=alpha_optimal)\n",
    "model_TextNB.fit(x_train_vect,y_train)\n",
    "predict_NB = model_TextNB.predict(x_test_vect)\n",
    "test_accNB = metrics.accuracy_score(y_test, predict_NB)\n",
    "print(test_accNB)\n",
    "\n",
    "score_NB = model_TextNB.score(x_test_vect, y_test)\n",
    "print(score_NB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736842105263\n",
      "0.736842105263\n"
     ]
    }
   ],
   "source": [
    "# Try a Tfid vectorizer instead of counts, still using bigrams\n",
    "vectimus_prime = TfidfVectorizer(preprocessor=text_cleaner, analyzer='word',ngram_range=(2,2)) \n",
    "x_train_vect = vectimus_prime.fit_transform(x_train_text)\n",
    "x_test_vect = vectimus_prime.transform(x_test_text)\n",
    "\n",
    "# Try Logistic Regression instead of Naive Bayes\n",
    "model_logR = LogisticRegression()\n",
    "model_logR.fit(x_train_vect,y_train)\n",
    "predict_logR = model_logR.predict(x_test_vect)\n",
    "test_acclogR = metrics.accuracy_score(y_test,predict_logR)\n",
    "print(test_acclogR)\n",
    "\n",
    "score_logR = model_logR.score(x_test_vect, y_test)\n",
    "print(score_logR)\n",
    "\n",
    "# Accuracy score is exactly the same as the BernoulliNB model. That seems weird, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738390092879\n",
      "0.738390092879\n"
     ]
    }
   ],
   "source": [
    "# Try reducing the vocabulary to eliminate meaningless words.\n",
    "\n",
    "vectorsaurus = CountVectorizer(preprocessor=text_cleaner, analyzer='word', ngram_range=(2,2)) \n",
    "x_train_vect = vectorsaurus.fit_transform(x_train_text)\n",
    "x_test_vect = vectorsaurus.transform(x_test_text)\n",
    "\n",
    "# Determine weights with Logistic Regression and L1 regularization\n",
    "model_logR2 = LogisticRegression(penalty='l1')\n",
    "model_logR2_fit = model_logR2.fit(x_train_vect, y_train)\n",
    "\n",
    "# Create list of vocabulary words and their associated weights, then filter out everything with weight of zero.\n",
    "word_weights = dict(zip(vectorsaurus.vocabulary_.keys(),model_logR2_fit.coef_[0]))\n",
    "word_weights = dict((k, v) for k, v in word_weights.items() if v != 0)\n",
    "\n",
    "# Create new vocabulary without zero-weight features\n",
    "# new_vocab = { key: vectorsaurus.vocabulary_[key] for key in word_weights.keys() }\n",
    "new_vocab = list(word_weights.keys())\n",
    "\n",
    "# Re-run the vectorization, and run the models again using the new data\n",
    "vectorsaurus_rex = CountVectorizer(preprocessor=text_cleaner, vocabulary=new_vocab)\n",
    "x_train_vect = vectorsaurus_rex.fit_transform(x_train_text)\n",
    "x_test_vect = vectorsaurus_rex.transform(x_test_text)\n",
    "\n",
    "model_logR3 = LogisticRegression()\n",
    "model_logR3_fit = model_logR3.fit(x_train_vect, y_train)\n",
    "predict_logR3 = model_logR3_fit.predict(x_test_vect)\n",
    "test_acclogR3 = metrics.accuracy_score(y_test,predict_logR3)\n",
    "\n",
    "print(test_acclogR3)\n",
    "\n",
    "score_logR3 = model_logR3_fit.score(x_test_vect, y_test)\n",
    "print(score_logR3)\n",
    "# Ok, this is getting weird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diseases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-115a23b16de7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#        diseases[row[0].lower()] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdisease_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiseases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdisease_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sick\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdisease_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doctor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diseases' is not defined"
     ]
    }
   ],
   "source": [
    "# try reducing vocab down to disease words\n",
    "\n",
    "# load in disease vocab\n",
    "import csv\n",
    "disease_list = []\n",
    "\n",
    "#with open('../data/diseases.csv', mode='r') as fin:\n",
    "#    reader = csv.reader(fin)\n",
    "#    diseases = {}\n",
    "#    for row in reader:\n",
    "#        diseases[row[0].lower()] = 0\n",
    "    \n",
    "disease_list = list(diseases.keys())\n",
    "disease_list.append(\"sick\")\n",
    "disease_list.append(\"doctor\")\n",
    "disease_list.append(\"doctors\")\n",
    "disease_list.append(\"dying\")\n",
    "disease_list.append(\"died\")\n",
    "disease_list.append(\"hospice\")\n",
    "disease_list.append(\"pain\")\n",
    "disease_list.append(\"medical\")\n",
    "disease_list.append(\"insurance\")\n",
    "\n",
    "# Set up count vectorizer to use custom preprocessor\n",
    "vectotron = CountVectorizer(vocabulary=disease_list, preprocessor=text_cleaner, analyzer='word',ngram_range=(2,2)) \n",
    "x_train_vect = vectotron.fit_transform(x_train_text)\n",
    "x_test_vect = vectotron.transform(x_test_text)\n",
    "\n",
    "# Fit a Bernoulli Naive Bayes model using the vectorized text and use GridSearch to optimize params\n",
    "model_TextNB = BernoulliNB()\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 50.0, 100.0]}\n",
    "BernNB_clf = GridSearchCV(model_TextNB,param_grid=alphas)\n",
    "BernNB_clf_fit = BernNB_clf.fit(x_train_vect,y_train)\n",
    "# print('Optimized score for BernoulliNB (alpha=',BernNB_clf.best_params_['alpha'],'): ',BernNB_clf.best_score_,'\\n',sep='')\n",
    "alpha_optimal = BernNB_clf_fit.best_params_['alpha']\n",
    "\n",
    "# Predict and check accuracy\n",
    "model_TextNB = BernoulliNB(alpha=alpha_optimal)\n",
    "model_TextNB_fit = model_TextNB.fit(x_train_vect,y_train)\n",
    "predict_NB = model_TextNB_fit.predict(x_test_vect)\n",
    "test_accNB = metrics.accuracy_score(y_test,predict_NB)\n",
    "\n",
    "print(test_accNB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
